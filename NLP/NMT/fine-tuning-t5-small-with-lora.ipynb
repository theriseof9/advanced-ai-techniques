{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.12",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30627,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Overview\n",
    "\n",
    "**Note: All the images are from the Reference lists section or the internet**\n",
    "\n",
    "We are going to fine-tuning a t5-small quantized model for English to French translation-related tasks using the LoRA method. We use the peft and transformers libraries for training.\n",
    "\n",
    "\n",
    "# About quantization\n",
    "\n",
    "Please check the list below:\n",
    "\n",
    "* [Quantization Technologies](https://www.kaggle.com/code/aisuko/quantization-technologies)\n",
    "* [Zero Degradation matrix multiplication](https://www.kaggle.com/code/aisuko/zero-degradation-matrix-multiplication)\n",
    "* [Lighter models on GPU for inference](https://www.kaggle.com/code/aisuko/lighter-models-on-gpu-for-inference)\n",
    "\n",
    "\n",
    "# About LoRA(Low Rank Adaptation)\n",
    "\n",
    "> A technique that accelerates the fine-tuning of large models while consuming less memory.\n",
    "\n",
    "\n",
    "**The idea is to freeze the original pre-trained weights(Matrices) and introduce new updata matrices**. These new matrics are trained on new data while keeping the overall number of changes low. The original weights matrix doesn't receive any adjustments. And finally, both the original and the adapted weights are combined.\n",
    "\n",
    "![](https://files.mastodon.social/media_attachments/files/111/702/004/494/881/797/original/a26697e010f0096b.webp)\n",
    "\n",
    "LoRA makes fine-tuning more efficient by drastically reducing the number of **trainable parameters**. In principle, LoRA can be applied to any subset of weight matrices in a neural network to reduce the number of trainable parameters. However, for simplicity and further parameter efficiency, **in Transformer models LoRA is typically applied to attention blocks only**. **The resulting number of trainable parameters in a LoRA model depends on the size of the low-rank update matrices, which is determined manily by the rank `r` and the shape of the original weight matrix**.\n",
    "\n",
    "\n",
    "The differences between QLoRA and LoRA in real word case see notebook [fine-tuning llama2 with QLoRA](https://www.kaggle.com/code/aisuko/fine-tuning-llama2-with-qlora?scriptVersionId=158763163&cellId=1)."
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "%%capture\n",
    "!pip install transformers==4.36.2\n",
    "!pip install bitsandbytes==0.41.3\n",
    "!pip install accelerate==0.25.0\n",
    "!pip install datasets==2.15.0\n",
    "!pip install evaluate==0.4.1\n",
    "!pip install peft==0.7.1"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T05:27:37.239972Z",
     "iopub.execute_input": "2024-06-18T05:27:37.240411Z",
     "iopub.status.idle": "2024-06-18T05:29:20.101175Z",
     "shell.execute_reply.started": "2024-06-18T05:27:37.240376Z",
     "shell.execute_reply": "2024-06-18T05:29:20.099828Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import os\n",
    "from huggingface_hub import login\n",
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "user_secrets = UserSecretsClient()\n",
    "\n",
    "login(token=\"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\")\n",
    "\n",
    "os.environ[\"WANDB_API_KEY\"]=\"b47babba2becb2d7866813aeb59313346b518185\"\n",
    "os.environ[\"WANDB_PROJECT\"] = \"Fine-tuning t5-small-on-opus100\"\n",
    "os.environ[\"WANDB_NAME\"] = \"ft-t5-small-on-opus100\""
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T05:29:20.103548Z",
     "iopub.execute_input": "2024-06-18T05:29:20.103905Z",
     "iopub.status.idle": "2024-06-18T05:29:20.681621Z",
     "shell.execute_reply.started": "2024-06-18T05:29:20.103876Z",
     "shell.execute_reply": "2024-06-18T05:29:20.680525Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "text": "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Loading Dataset\n",
    "\n",
    "We are going to use the optus 100 dataset for training which gives us access to more than 100 different languages."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import get_dataset_config_names\n",
    "\n",
    "configs=get_dataset_config_names(\"opus100\")\n",
    "print(configs)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T05:29:20.684038Z",
     "iopub.execute_input": "2024-06-18T05:29:20.684974Z",
     "iopub.status.idle": "2024-06-18T05:29:39.272871Z",
     "shell.execute_reply.started": "2024-06-18T05:29:20.684929Z",
     "shell.execute_reply": "2024-06-18T05:29:39.271826Z"
    },
    "trusted": true
   },
   "execution_count": 3,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading readme:   0%|          | 0.00/65.4k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "77f6aff4fc7e438d8df69c10ff9ddc76"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "['af-en', 'am-en', 'an-en', 'ar-de', 'ar-en', 'ar-fr', 'ar-nl', 'ar-ru', 'ar-zh', 'as-en', 'az-en', 'be-en', 'bg-en', 'bn-en', 'br-en', 'bs-en', 'ca-en', 'cs-en', 'cy-en', 'da-en', 'de-en', 'de-fr', 'de-nl', 'de-ru', 'de-zh', 'dz-en', 'el-en', 'en-eo', 'en-es', 'en-et', 'en-eu', 'en-fa', 'en-fi', 'en-fr', 'en-fy', 'en-ga', 'en-gd', 'en-gl', 'en-gu', 'en-ha', 'en-he', 'en-hi', 'en-hr', 'en-hu', 'en-hy', 'en-id', 'en-ig', 'en-is', 'en-it', 'en-ja', 'en-ka', 'en-kk', 'en-km', 'en-kn', 'en-ko', 'en-ku', 'en-ky', 'en-li', 'en-lt', 'en-lv', 'en-mg', 'en-mk', 'en-ml', 'en-mn', 'en-mr', 'en-ms', 'en-mt', 'en-my', 'en-nb', 'en-ne', 'en-nl', 'en-nn', 'en-no', 'en-oc', 'en-or', 'en-pa', 'en-pl', 'en-ps', 'en-pt', 'en-ro', 'en-ru', 'en-rw', 'en-se', 'en-sh', 'en-si', 'en-sk', 'en-sl', 'en-sq', 'en-sr', 'en-sv', 'en-ta', 'en-te', 'en-tg', 'en-th', 'en-tk', 'en-tr', 'en-tt', 'en-ug', 'en-uk', 'en-ur', 'en-uz', 'en-vi', 'en-wa', 'en-xh', 'en-yi', 'en-yo', 'en-zh', 'en-zu', 'fr-nl', 'fr-ru', 'fr-zh', 'nl-ru', 'nl-zh', 'ru-zh']\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "We will use the \"en-fr\" data for language translation. Let's download and load the dataset though the `load_dataset`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "dataset=load_dataset(\"opus100\", \"en-fr\")\n",
    "dataset"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T05:29:39.275938Z",
     "iopub.execute_input": "2024-06-18T05:29:39.276601Z",
     "iopub.status.idle": "2024-06-18T05:29:52.238155Z",
     "shell.execute_reply.started": "2024-06-18T05:29:39.276562Z",
     "shell.execute_reply": "2024-06-18T05:29:52.237129Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.24.3\n  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading data files:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6a894a8855d24a8dbcb8fb547d2363cf"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/327k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e7d70dadc54843d0b6033bb76b3e4265"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/142M [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "465b4c61be354d65aefe9e7ebe328be8"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading data:   0%|          | 0.00/334k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "8386a2775ed047ccbb077c0ace42bf24"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Extracting data files:   0%|          | 0/3 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ab44458d52e40ad8093aeedd7daed14"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7092325cbcb84856ba304322ce74779f"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating train split:   0%|          | 0/1000000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f3ee2930ce1435c8c82f8710e5b53e6"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "409490bcbd4f4d84b4ecd593c9f5701b"
      }
     },
     "metadata": {}
    },
    {
     "execution_count": 4,
     "output_type": "execute_result",
     "data": {
      "text/plain": "DatasetDict({\n    test: Dataset({\n        features: ['translation'],\n        num_rows: 2000\n    })\n    train: Dataset({\n        features: ['translation'],\n        num_rows: 1000000\n    })\n    validation: Dataset({\n        features: ['translation'],\n        num_rows: 2000\n    })\n})"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data Tokenization"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "model_name=\"google/madlad40-03b-mt\"\n",
    "prompt=\"My name is Kaggle, nice to see you.\"\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name, load_in_8bit=True, device_map=\"auto\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T05:56:26.045061Z",
     "iopub.execute_input": "2024-06-18T05:56:26.045506Z",
     "iopub.status.idle": "2024-06-18T05:56:26.267990Z",
     "shell.execute_reply.started": "2024-06-18T05:56:26.045473Z",
     "shell.execute_reply": "2024-06-18T05:56:26.265247Z"
    },
    "trusted": true
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# use a sample of around 2000 instead of the complete dataset as training dataset\n",
    "train_dataset=dataset['train'].shuffle(seed=42).select(range(2000))\n",
    "\n",
    "# as evaluation dataset\n",
    "eval_dataset=dataset['validation']\n",
    "\n",
    "\n",
    "def preprocess_func(data):\n",
    "    inputs=[ex['en'] for ex in data['translation']]\n",
    "    targets=[ex['fr'] for ex in data['translation']]\n",
    "    \n",
    "    # tokenize each row of inputs and outputs\n",
    "    model_inputs=tokenizer(inputs, truncation=True)\n",
    "    labels=tokenizer(targets, truncation=True)\n",
    "    \n",
    "    model_inputs[\"labels\"]=labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# We tokenize the entire dataset\n",
    "train_dataset=train_dataset.map(preprocess_func, batched=True)\n",
    "eval_dataset=eval_dataset.map(preprocess_func, batched=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T05:56:26.286125Z",
     "iopub.execute_input": "2024-06-18T05:56:26.286565Z",
     "iopub.status.idle": "2024-06-18T05:56:27.090311Z",
     "shell.execute_reply.started": "2024-06-18T05:56:26.286527Z",
     "shell.execute_reply": "2024-06-18T05:56:27.089109Z"
    },
    "trusted": true
   },
   "execution_count": 22,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Map:   0%|          | 0/2000 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7d08d2653a0748249fab9ac11c0db36a"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Preparing the model\n",
    "\n",
    "First, we will load the model with 8 bit(quantization the model). And then, we are using the LoRA. Here are the description of the LoraConfig:\n",
    "\n",
    "* **r**: the rank of the update matrices, expressed in **int**. Lower rank results in smaller update matrices with fewer trainable parameters.\n",
    "* **target_modules**: **The modules(for example, attention blocks)** to apply the LoRA update matrices.\n",
    "* **alpha**: LoRA scaling factor\n",
    "* **bias**: Specifies if the bias parameters should be trained. Can be 'none','all' or 'lora_only'.\n",
    "* **module_to_save**: List of modules apart from LoRA layers to be set as trainable and saved in the final checkpoint. These typically include model's custome head that is randomly initialized for the fine-tuning task.\n",
    "* **layer_to_transform**: List of layers to be transformed by LoRA. If not specified, all layers in `target_modules` are transformed.\n",
    "* **layers_pattern**: Pattern to match layer names in `target_modules`, if `layer_to_transform` is specified. By default `PeftModel` will look at common layer pattern(`layers`,`h`, `blocks`, etc.), use it for exotic and custom models.\n",
    "* **rank_pattern**: The mapping from layer names or regexp expression to ranks which are different from the default tank specified by `r`.\n",
    "* **alpha_pattern**: The mapping from layer names or regexp expression to alphas which are different from the default alpha specified by `lora_alpha`."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from peft import PeftModel, prepare_model_for_kbit_training, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "    load_in_8bit=True\n",
    ")\n",
    "\n",
    "model=AutoModelForSeq2SeqLM.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T05:56:27.092824Z",
     "iopub.execute_input": "2024-06-18T05:56:27.093303Z",
     "iopub.status.idle": "2024-06-18T05:56:31.110810Z",
     "shell.execute_reply.started": "2024-06-18T05:56:27.093257Z",
     "shell.execute_reply": "2024-06-18T05:56:31.109620Z"
    },
    "trusted": true
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Freeze the original parameters\n",
    "model=prepare_model_for_kbit_training(model)\n",
    "\n",
    "peft_config=LoraConfig(\n",
    "    # the task to train for (sequence-to-sequence language modeling in this case)\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    # the dimension of the low-rank matrices\n",
    "    r=4,\n",
    "    # the scaling factor for the low-rank matrices\n",
    "    lora_alpha=64,\n",
    "    # the dropout probability of the LoRA layers\n",
    "    lora_dropout=0.01,\n",
    "    target_modules=[\"k\",\"q\",\"v\",\"o\"],\n",
    ")\n",
    "\n",
    "peft_model=get_peft_model(model, peft_config)\n",
    "peft_model.print_trainable_parameters()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T05:56:31.112682Z",
     "iopub.execute_input": "2024-06-18T05:56:31.113369Z",
     "iopub.status.idle": "2024-06-18T05:56:31.533313Z",
     "shell.execute_reply.started": "2024-06-18T05:56:31.113331Z",
     "shell.execute_reply": "2024-06-18T05:56:31.532004Z"
    },
    "trusted": true
   },
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "text": "trainable params: 2,359,296 || all params: 785,509,376 || trainable%: 0.30035236651331837\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy=evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels=p\n",
    "    predictions=np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T05:56:31.536244Z",
     "iopub.execute_input": "2024-06-18T05:56:31.536674Z",
     "iopub.status.idle": "2024-06-18T05:56:31.807046Z",
     "shell.execute_reply.started": "2024-06-18T05:56:31.536621Z",
     "shell.execute_reply": "2024-06-18T05:56:31.805988Z"
    },
    "trusted": true
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ttraining"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# ignore tokenizer pad token in the loss\n",
    "label_pad_token_id=-100\n",
    "\n",
    "# padding the sentence of the entire datasets\n",
    "data_collator=DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer, \n",
    "    model=peft_model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T05:56:31.808946Z",
     "iopub.execute_input": "2024-06-18T05:56:31.809429Z",
     "iopub.status.idle": "2024-06-18T05:56:31.817431Z",
     "shell.execute_reply.started": "2024-06-18T05:56:31.809374Z",
     "shell.execute_reply": "2024-06-18T05:56:31.816213Z"
    },
    "trusted": true
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "training_args=Seq2SeqTrainingArguments(\n",
    "    output_dir=os.getenv(\"WANDB_NAME\"),\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=1,\n",
    "    logging_dir=os.getenv(\"WANDB_NAME\")+\"/logs\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    logging_steps=500,\n",
    "    save_strategy=\"epoch\",\n",
    "    push_to_hub=False,\n",
    "    report_to=\"wandb\",\n",
    "    run_name=os.getenv(\"WANDB_NAME\"),\n",
    ")\n",
    "\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer=Seq2SeqTrainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "peft_model.config.use_cache=False\n",
    "trainer.train()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T05:56:31.818958Z",
     "iopub.execute_input": "2024-06-18T05:56:31.819847Z",
     "iopub.status.idle": "2024-06-18T06:20:01.218260Z",
     "shell.execute_reply.started": "2024-06-18T05:56:31.819808Z",
     "shell.execute_reply": "2024-06-18T06:20:01.216968Z"
    },
    "trusted": true
   },
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "text": "You're using a T5TokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "\n    <div>\n      \n      <progress value='315' max='315' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [315/315 23:23, Epoch 5/5]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>63</td>\n      <td>1.953500</td>\n    </tr>\n    <tr>\n      <td>126</td>\n      <td>1.771600</td>\n    </tr>\n    <tr>\n      <td>189</td>\n      <td>1.718900</td>\n    </tr>\n    <tr>\n      <td>252</td>\n      <td>1.655900</td>\n    </tr>\n    <tr>\n      <td>315</td>\n      <td>1.617600</td>\n    </tr>\n  </tbody>\n</table><p>"
     },
     "metadata": {}
    },
    {
     "name": "stderr",
     "text": "Checkpoint destination directory ft-t5-small-on-opus100/checkpoint-63 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\nCheckpoint destination directory ft-t5-small-on-opus100/checkpoint-126 already exists and is non-empty.Saving will proceed but saved results may be invalid.\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n/opt/conda/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py:322: UserWarning: MatMul8bitLt: inputs will be cast from torch.float32 to float16 during quantization\n  warnings.warn(f\"MatMul8bitLt: inputs will be cast from {A.dtype} to float16 during quantization\")\n",
     "output_type": "stream"
    },
    {
     "execution_count": 27,
     "output_type": "execute_result",
     "data": {
      "text/plain": "TrainOutput(global_step=315, training_loss=1.7435093470982144, metrics={'train_runtime': 1408.4177, 'train_samples_per_second': 7.1, 'train_steps_per_second': 0.224, 'total_flos': 4736164178165760.0, 'train_loss': 1.7435093470982144, 'epoch': 5.0})"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "del train_dataset"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T06:20:01.219394Z",
     "iopub.execute_input": "2024-06-18T06:20:01.219706Z",
     "iopub.status.idle": "2024-06-18T06:20:01.226556Z",
     "shell.execute_reply.started": "2024-06-18T06:20:01.219680Z",
     "shell.execute_reply": "2024-06-18T06:20:01.224865Z"
    },
    "trusted": true
   },
   "execution_count": 28,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# trainer.evaluate() # out of GPU memory"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T05:47:30.114432Z",
     "iopub.execute_input": "2024-06-18T05:47:30.114884Z",
     "iopub.status.idle": "2024-06-18T05:48:09.622385Z",
     "shell.execute_reply.started": "2024-06-18T05:47:30.114849Z",
     "shell.execute_reply": "2024-06-18T05:48:09.619249Z"
    },
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "trusted": true
   },
   "execution_count": 15,
   "outputs": [
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mOutOfMemoryError\u001B[0m                          Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[15], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[43mtrainer\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;66;03m# out of GPU memory\u001B[39;00m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_seq2seq.py:166\u001B[0m, in \u001B[0;36mSeq2SeqTrainer.evaluate\u001B[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix, **gen_kwargs)\u001B[0m\n\u001B[1;32m    164\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgather_function \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39maccelerator\u001B[38;5;241m.\u001B[39mgather\n\u001B[1;32m    165\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_gen_kwargs \u001B[38;5;241m=\u001B[39m gen_kwargs\n\u001B[0;32m--> 166\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mevaluate\u001B[49m\u001B[43m(\u001B[49m\u001B[43meval_dataset\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mignore_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3019\u001B[0m, in \u001B[0;36mTrainer.evaluate\u001B[0;34m(self, eval_dataset, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[1;32m   3016\u001B[0m start_time \u001B[38;5;241m=\u001B[39m time\u001B[38;5;241m.\u001B[39mtime()\n\u001B[1;32m   3018\u001B[0m eval_loop \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mprediction_loop \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39muse_legacy_prediction_loop \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mevaluation_loop\n\u001B[0;32m-> 3019\u001B[0m output \u001B[38;5;241m=\u001B[39m \u001B[43meval_loop\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   3020\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_dataloader\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3021\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdescription\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mEvaluation\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3022\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# No point gathering the predictions if there are no metrics, otherwise we defer to\u001B[39;49;00m\n\u001B[1;32m   3023\u001B[0m \u001B[43m    \u001B[49m\u001B[38;5;66;43;03m# self.args.prediction_loss_only\u001B[39;49;00m\n\u001B[1;32m   3024\u001B[0m \u001B[43m    \u001B[49m\u001B[43mprediction_loss_only\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mif\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcompute_metrics\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01mis\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01melse\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mNone\u001B[39;49;00m\u001B[43m,\u001B[49m\n\u001B[1;32m   3025\u001B[0m \u001B[43m    \u001B[49m\u001B[43mignore_keys\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mignore_keys\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3026\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmetric_key_prefix\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   3027\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3029\u001B[0m total_batch_size \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39meval_batch_size \u001B[38;5;241m*\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mworld_size\n\u001B[1;32m   3030\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmetric_key_prefix\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_jit_compilation_time\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01min\u001B[39;00m output\u001B[38;5;241m.\u001B[39mmetrics:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:3234\u001B[0m, in \u001B[0;36mTrainer.evaluation_loop\u001B[0;34m(self, dataloader, description, prediction_loss_only, ignore_keys, metric_key_prefix)\u001B[0m\n\u001B[1;32m   3232\u001B[0m         logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpreprocess_logits_for_metrics(logits, labels)\n\u001B[1;32m   3233\u001B[0m     logits \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgather_function((logits))\n\u001B[0;32m-> 3234\u001B[0m     preds_host \u001B[38;5;241m=\u001B[39m logits \u001B[38;5;28;01mif\u001B[39;00m preds_host \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m \u001B[43mnested_concat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mpreds_host\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mlogits\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;241;43m-\u001B[39;49m\u001B[38;5;241;43m100\u001B[39;49m\u001B[43m)\u001B[49m\n\u001B[1;32m   3236\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m labels \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   3237\u001B[0m     labels \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mgather_function((labels))\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:121\u001B[0m, in \u001B[0;36mnested_concat\u001B[0;34m(tensors, new_tensors, padding_index)\u001B[0m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(tensors) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mtype\u001B[39m(\n\u001B[1;32m    118\u001B[0m     new_tensors\n\u001B[1;32m    119\u001B[0m ), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(tensors)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(new_tensors)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tensors, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m--> 121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mtype\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m)\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnested_concat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding_index\u001B[49m\u001B[43m)\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mfor\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;129;43;01min\u001B[39;49;00m\u001B[43m \u001B[49m\u001B[38;5;28;43mzip\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnew_tensors\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tensors, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m    123\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001B[38;5;241m=\u001B[39mpadding_index)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:121\u001B[0m, in \u001B[0;36m<genexpr>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    117\u001B[0m \u001B[38;5;28;01massert\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(tensors) \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mtype\u001B[39m(\n\u001B[1;32m    118\u001B[0m     new_tensors\n\u001B[1;32m    119\u001B[0m ), \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mExpected `tensors` and `new_tensors` to have the same type but found \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(tensors)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m and \u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mtype\u001B[39m(new_tensors)\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    120\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tensors, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m--> 121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(tensors)(\u001B[43mnested_concat\u001B[49m\u001B[43m(\u001B[49m\u001B[43mt\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding_index\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m t, n \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(tensors, new_tensors))\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tensors, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[1;32m    123\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m torch_pad_and_concatenate(tensors, new_tensors, padding_index\u001B[38;5;241m=\u001B[39mpadding_index)\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:123\u001B[0m, in \u001B[0;36mnested_concat\u001B[0;34m(tensors, new_tensors, padding_index)\u001B[0m\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(tensors)(nested_concat(t, n, padding_index\u001B[38;5;241m=\u001B[39mpadding_index) \u001B[38;5;28;01mfor\u001B[39;00m t, n \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mzip\u001B[39m(tensors, new_tensors))\n\u001B[1;32m    122\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tensors, torch\u001B[38;5;241m.\u001B[39mTensor):\n\u001B[0;32m--> 123\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mtorch_pad_and_concatenate\u001B[49m\u001B[43m(\u001B[49m\u001B[43mtensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mnew_tensors\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_index\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpadding_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    124\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(tensors, Mapping):\n\u001B[1;32m    125\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mtype\u001B[39m(tensors)(\n\u001B[1;32m    126\u001B[0m         {k: nested_concat(t, new_tensors[k], padding_index\u001B[38;5;241m=\u001B[39mpadding_index) \u001B[38;5;28;01mfor\u001B[39;00m k, t \u001B[38;5;129;01min\u001B[39;00m tensors\u001B[38;5;241m.\u001B[39mitems()}\n\u001B[1;32m    127\u001B[0m     )\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_pt_utils.py:88\u001B[0m, in \u001B[0;36mtorch_pad_and_concatenate\u001B[0;34m(tensor1, tensor2, padding_index)\u001B[0m\n\u001B[1;32m     85\u001B[0m new_shape \u001B[38;5;241m=\u001B[39m (tensor1\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m+\u001B[39m tensor2\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], \u001B[38;5;28mmax\u001B[39m(tensor1\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m], tensor2\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m])) \u001B[38;5;241m+\u001B[39m tensor1\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m2\u001B[39m:]\n\u001B[1;32m     87\u001B[0m \u001B[38;5;66;03m# Now let's fill the result tensor\u001B[39;00m\n\u001B[0;32m---> 88\u001B[0m result \u001B[38;5;241m=\u001B[39m \u001B[43mtensor1\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mnew_full\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnew_shape\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mpadding_index\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     89\u001B[0m result[: tensor1\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m], : tensor1\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]] \u001B[38;5;241m=\u001B[39m tensor1\n\u001B[1;32m     90\u001B[0m result[tensor1\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m0\u001B[39m] :, : tensor2\u001B[38;5;241m.\u001B[39mshape[\u001B[38;5;241m1\u001B[39m]] \u001B[38;5;241m=\u001B[39m tensor2\n",
      "\u001B[0;31mOutOfMemoryError\u001B[0m: CUDA out of memory. Tried to allocate 2.82 GiB (GPU 0; 15.89 GiB total capacity; 10.94 GiB already allocated; 2.83 GiB free; 12.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ],
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 2.82 GiB (GPU 0; 15.89 GiB total capacity; 10.94 GiB already allocated; 2.83 GiB free; 12.77 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "kwargs={\n",
    "    'model_name': f'{os.getenv(\"WANDB_NAME\")}',\n",
    "    'finetuned_from': model_name,\n",
    "    'tasks': 'Translation',\n",
    "#     'dataset_tags':'',\n",
    "    'dataset':'opus100'\n",
    "}\n",
    "\n",
    "tokenizer.push_to_hub(os.getenv(\"WANDB_NAME\"))\n",
    "trainer.push_to_hub(**kwargs)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T05:04:04.179122Z",
     "iopub.status.idle": "2024-06-18T05:04:04.179427Z",
     "shell.execute_reply.started": "2024-06-18T05:04:04.179268Z",
     "shell.execute_reply": "2024-06-18T05:04:04.179282Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Inference"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "peft_model.config.use_cache=True\n",
    "context=tokenizer([\"Hello\"], return_tensors=\"pt\")\n",
    "output=peft_model.generate(**context)\n",
    "\n",
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T06:24:13.111362Z",
     "iopub.execute_input": "2024-06-18T06:24:13.111730Z",
     "iopub.status.idle": "2024-06-18T06:24:17.278900Z",
     "shell.execute_reply.started": "2024-06-18T06:24:13.111699Z",
     "shell.execute_reply": "2024-06-18T06:24:17.277865Z"
    },
    "trusted": true
   },
   "execution_count": 34,
   "outputs": [
    {
     "execution_count": 34,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'- Vous ne êtes pas en dehors de la ville '"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from peft import PeftModel, PeftConfig\n",
    "from transformers import AutoModelForSeq2SeqLM\n",
    "\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-small\")\n",
    "model = PeftModel.from_pretrained(model, \"aisuko/ft-t5-small-on-opus100\")\n",
    "\n",
    "output=model.generate(**context)\n",
    "tokenizer.decode(output[0], skip_special_tokens=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T05:04:04.184449Z",
     "iopub.status.idle": "2024-06-18T05:04:04.184844Z",
     "shell.execute_reply.started": "2024-06-18T05:04:04.184647Z",
     "shell.execute_reply": "2024-06-18T05:04:04.184664Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# References List\n",
    "\n",
    "* https://towardsdev.com/fine-tune-quantized-language-model-using-lora-with-peft-transformers-on-t4-gpu-287da2d5d7f1\n",
    "* https://huggingface.co/docs/peft/quicktour"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}

{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30733,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "code",
   "source": [
    "!pip install -i https://pypi.org/simple/ bitsandbytes\n",
    "!pip install transformers datasets sacrebleu"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T12:27:22.235611Z",
     "iopub.execute_input": "2024-06-18T12:27:22.236316Z",
     "iopub.status.idle": "2024-06-18T12:27:49.458970Z",
     "shell.execute_reply.started": "2024-06-18T12:27:22.236283Z",
     "shell.execute_reply": "2024-06-18T12:27:49.457715Z"
    },
    "trusted": true
   },
   "execution_count": 70,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Looking in indexes: https://pypi.org/simple/\nRequirement already satisfied: bitsandbytes in /opt/conda/lib/python3.10/site-packages (0.43.1)\nRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (2.1.2)\nRequirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from bitsandbytes) (1.26.4)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.13.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (4.9.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (3.1.2)\nRequirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->bitsandbytes) (2024.3.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->bitsandbytes) (2.1.3)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->bitsandbytes) (1.3.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.41.2)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.19.2)\nRequirement already satisfied: sacrebleu in /opt/conda/lib/python3.10/site-packages (2.4.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.1)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.3.1,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.3.1,>=2023.1.0->datasets) (2024.3.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: portalocker in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (2.8.2)\nRequirement already satisfied: tabulate>=0.8.9 in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: colorama in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /opt/conda/lib/python3.10/site-packages (from sacrebleu) (5.2.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.0->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "from transformers import MT5ForConditionalGeneration, MT5Tokenizer\n",
    "from datasets import load_dataset, load_metric\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "bnb_config=BitsAndBytesConfig(\n",
    "    load_in_8bit=True\n",
    ")\n",
    "\n",
    "# Load the mT5 model and tokenizer\n",
    "model_name = \"google/mt5-large\"\n",
    "model = MT5ForConditionalGeneration.from_pretrained(model_name, quantization_config=bnb_config, device_map=\"auto\")\n",
    "tokenizer = MT5Tokenizer.from_pretrained(model_name)"
   ],
   "metadata": {
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "execution": {
     "iopub.status.busy": "2024-06-18T13:08:53.951804Z",
     "iopub.execute_input": "2024-06-18T13:08:53.952310Z",
     "iopub.status.idle": "2024-06-18T13:09:06.028310Z",
     "shell.execute_reply.started": "2024-06-18T13:08:53.952274Z",
     "shell.execute_reply": "2024-06-18T13:09:06.027262Z"
    },
    "trusted": true
   },
   "execution_count": 105,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.2/metrics/sacrebleu/sacrebleu.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "# Load the OPUS-100 dataset\n",
    "dataset_test = load_dataset(\"wmt14\", \"de-en\", split=\"test\")\n",
    "dataset_train = load_dataset(\"wmt14\", \"de-en\", split=\"train\")\n",
    "dataset_validation = load_dataset(\"wmt14\", \"de-en\", split=\"validation\")\n",
    "\n",
    "# Load the BLEU metric\n",
    "bleu = load_metric(\"sacrebleu\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T13:22:10.427338Z",
     "iopub.execute_input": "2024-06-18T13:22:10.428154Z",
     "iopub.status.idle": "2024-06-18T13:22:16.699251Z",
     "shell.execute_reply.started": "2024-06-18T13:22:10.428113Z",
     "shell.execute_reply": "2024-06-18T13:22:16.697758Z"
    },
    "trusted": true
   },
   "execution_count": 112,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/site-packages/datasets/load.py:759: FutureWarning: The repository for sacrebleu contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.19.2/metrics/sacrebleu/sacrebleu.py\nYou can avoid this message in future by passing the argument `trust_remote_code=True`.\nPassing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n  warnings.warn(\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T12:27:51.478725Z",
     "iopub.status.idle": "2024-06-18T12:27:51.479134Z",
     "shell.execute_reply.started": "2024-06-18T12:27:51.478933Z",
     "shell.execute_reply": "2024-06-18T12:27:51.478949Z"
    },
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install peft -U\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T13:31:39.433043Z",
     "iopub.execute_input": "2024-06-18T13:31:39.433460Z",
     "iopub.status.idle": "2024-06-18T13:31:54.949187Z",
     "shell.execute_reply.started": "2024-06-18T13:31:39.433430Z",
     "shell.execute_reply": "2024-06-18T13:31:54.947950Z"
    },
    "trusted": true
   },
   "execution_count": 145,
   "outputs": [
    {
     "name": "stderr",
     "text": "/opt/conda/lib/python3.10/pty.py:89: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  pid, fd = os.forkpty()\n",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "Requirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.7.1)\nCollecting peft\n  Downloading peft-0.11.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (6.0.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.1.2)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.41.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.66.4)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.30.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.4.3)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.23.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.13.1)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2024.3.1)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.32.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.1.1)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12.1)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.12.25)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.19.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.3)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2024.2.2)\nRequirement already satisfied: mpmath<1.4.0,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\nDownloading peft-0.11.1-py3-none-any.whl (251 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m251.6/251.6 kB\u001B[0m \u001B[31m4.5 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n\u001B[?25hInstalling collected packages: peft\n  Attempting uninstall: peft\n    Found existing installation: peft 0.7.1\n    Uninstalling peft-0.7.1:\n      Successfully uninstalled peft-0.7.1\nSuccessfully installed peft-0.11.1\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from peft import PeftModel, prepare_model_for_kbit_training, PeftConfig, get_peft_model, LoraConfig, TaskType"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T13:24:04.375316Z",
     "iopub.execute_input": "2024-06-18T13:24:04.375713Z",
     "iopub.status.idle": "2024-06-18T13:24:04.380614Z",
     "shell.execute_reply.started": "2024-06-18T13:24:04.375681Z",
     "shell.execute_reply": "2024-06-18T13:24:04.379582Z"
    },
    "trusted": true
   },
   "execution_count": 117,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "peft_model"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T13:32:14.055213Z",
     "iopub.execute_input": "2024-06-18T13:32:14.056025Z",
     "iopub.status.idle": "2024-06-18T13:32:14.098685Z",
     "shell.execute_reply.started": "2024-06-18T13:32:14.055989Z",
     "shell.execute_reply": "2024-06-18T13:32:14.097557Z"
    },
    "trusted": true
   },
   "execution_count": 150,
   "outputs": [
    {
     "execution_count": 150,
     "output_type": "execute_result",
     "data": {
      "text/plain": "PeftModelForSeq2SeqLM(\n  (base_model): LoraModel(\n    (model): MT5ForConditionalGeneration(\n      (shared): Embedding(250112, 1024)\n      (encoder): MT5Stack(\n        (embed_tokens): Embedding(250112, 1024)\n        (block): ModuleList(\n          (0): MT5Block(\n            (layer): ModuleList(\n              (0): MT5LayerSelfAttention(\n                (SelfAttention): MT5Attention(\n                  (q): lora.Linear8bitLt(\n                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.01, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): lora.Linear8bitLt(\n                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.01, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (v): lora.Linear8bitLt(\n                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.01, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): lora.Linear8bitLt(\n                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.01, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (relative_attention_bias): Embedding(32, 16)\n                )\n                (layer_norm): MT5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): MT5LayerFF(\n                (DenseReluDense): MT5DenseGatedActDense(\n                  (wi_0): Linear8bitLt(in_features=1024, out_features=2816, bias=False)\n                  (wi_1): Linear8bitLt(in_features=1024, out_features=2816, bias=False)\n                  (wo): Linear(in_features=2816, out_features=1024, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): MT5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (1-23): 23 x MT5Block(\n            (layer): ModuleList(\n              (0): MT5LayerSelfAttention(\n                (SelfAttention): MT5Attention(\n                  (q): lora.Linear8bitLt(\n                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.01, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): lora.Linear8bitLt(\n                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.01, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (v): lora.Linear8bitLt(\n                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.01, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): lora.Linear8bitLt(\n                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.01, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                )\n                (layer_norm): MT5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): MT5LayerFF(\n                (DenseReluDense): MT5DenseGatedActDense(\n                  (wi_0): Linear8bitLt(in_features=1024, out_features=2816, bias=False)\n                  (wi_1): Linear8bitLt(in_features=1024, out_features=2816, bias=False)\n                  (wo): Linear(in_features=2816, out_features=1024, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): MT5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (final_layer_norm): MT5LayerNorm()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (decoder): MT5Stack(\n        (embed_tokens): Embedding(250112, 1024)\n        (block): ModuleList(\n          (0): MT5Block(\n            (layer): ModuleList(\n              (0): MT5LayerSelfAttention(\n                (SelfAttention): MT5Attention(\n                  (q): lora.Linear8bitLt(\n                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.01, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): lora.Linear8bitLt(\n                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.01, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (v): lora.Linear8bitLt(\n                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.01, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): lora.Linear8bitLt(\n                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.01, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (relative_attention_bias): Embedding(32, 16)\n                )\n                (layer_norm): MT5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): MT5LayerCrossAttention(\n                (EncDecAttention): MT5Attention(\n                  (q): lora.Linear8bitLt(\n                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.01, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): lora.Linear8bitLt(\n                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.01, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (v): lora.Linear8bitLt(\n                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.01, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): lora.Linear8bitLt(\n                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.01, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                )\n                (layer_norm): MT5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): MT5LayerFF(\n                (DenseReluDense): MT5DenseGatedActDense(\n                  (wi_0): Linear8bitLt(in_features=1024, out_features=2816, bias=False)\n                  (wi_1): Linear8bitLt(in_features=1024, out_features=2816, bias=False)\n                  (wo): Linear(in_features=2816, out_features=1024, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): MT5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n          (1-23): 23 x MT5Block(\n            (layer): ModuleList(\n              (0): MT5LayerSelfAttention(\n                (SelfAttention): MT5Attention(\n                  (q): lora.Linear8bitLt(\n                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.01, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): lora.Linear8bitLt(\n                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.01, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (v): lora.Linear8bitLt(\n                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.01, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): lora.Linear8bitLt(\n                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.01, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                )\n                (layer_norm): MT5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (1): MT5LayerCrossAttention(\n                (EncDecAttention): MT5Attention(\n                  (q): lora.Linear8bitLt(\n                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.01, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (k): lora.Linear8bitLt(\n                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.01, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (v): lora.Linear8bitLt(\n                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.01, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                  (o): lora.Linear8bitLt(\n                    (base_layer): Linear8bitLt(in_features=1024, out_features=1024, bias=False)\n                    (lora_dropout): ModuleDict(\n                      (default): Dropout(p=0.01, inplace=False)\n                    )\n                    (lora_A): ModuleDict(\n                      (default): Linear(in_features=1024, out_features=4, bias=False)\n                    )\n                    (lora_B): ModuleDict(\n                      (default): Linear(in_features=4, out_features=1024, bias=False)\n                    )\n                    (lora_embedding_A): ParameterDict()\n                    (lora_embedding_B): ParameterDict()\n                  )\n                )\n                (layer_norm): MT5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n              (2): MT5LayerFF(\n                (DenseReluDense): MT5DenseGatedActDense(\n                  (wi_0): Linear8bitLt(in_features=1024, out_features=2816, bias=False)\n                  (wi_1): Linear8bitLt(in_features=1024, out_features=2816, bias=False)\n                  (wo): Linear(in_features=2816, out_features=1024, bias=False)\n                  (dropout): Dropout(p=0.1, inplace=False)\n                  (act): NewGELUActivation()\n                )\n                (layer_norm): MT5LayerNorm()\n                (dropout): Dropout(p=0.1, inplace=False)\n              )\n            )\n          )\n        )\n        (final_layer_norm): MT5LayerNorm()\n        (dropout): Dropout(p=0.1, inplace=False)\n      )\n      (lm_head): Linear(in_features=1024, out_features=250112, bias=False)\n    )\n  )\n)"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Freeze the original parameters\n",
    "model=prepare_model_for_kbit_training(model)\n",
    "\n",
    "peft_config=LoraConfig(\n",
    "    # the task to train for (sequence-to-sequence language modeling in this case)\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
    "    # the dimension of the low-rank matrices\n",
    "    r=4,\n",
    "    # the scaling factor for the low-rank matrices\n",
    "    lora_alpha=64,\n",
    "    # the dropout probability of the LoRA layers\n",
    "    lora_dropout=0.01,\n",
    "    target_modules=[\"k\",\"q\",\"v\",\"o\"],\n",
    ")\n",
    "\n",
    "peft_model=get_peft_model(model, peft_config)\n",
    "peft_model.print_trainable_parameters()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T13:31:59.476251Z",
     "iopub.execute_input": "2024-06-18T13:31:59.476686Z",
     "iopub.status.idle": "2024-06-18T13:31:59.916248Z",
     "shell.execute_reply.started": "2024-06-18T13:31:59.476649Z",
     "shell.execute_reply": "2024-06-18T13:31:59.915065Z"
    },
    "trusted": true
   },
   "execution_count": 147,
   "outputs": [
    {
     "name": "stdout",
     "text": "trainable params: 2,359,296 || all params: 1,231,940,608 || trainable%: 0.19151053100118282\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "prefix = \"translate English to German: \"\n",
    "def preprocess_func(data):\n",
    "    inputs = [prefix + ex['en'] for ex in data['translation']]\n",
    "    targets = [ex['de'] for ex in data['translation']]\n",
    "    \n",
    "    # Tokenize each row of inputs and outputs with padding\n",
    "    model_inputs = tokenizer(inputs, truncation=True, padding=\"max_length\", max_length=128)\n",
    "    labels = tokenizer(targets, truncation=True, padding=\"max_length\", max_length=128)\n",
    "    \n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "\n",
    "# Preprocess the dataset\n",
    "processed_dataset_train = dataset_train.shuffle(seed=42).select(range(5000)).map(preprocess_func, batched=True)\n",
    "processed_dataset_test = dataset_test.select(range(2)).map(preprocess_func, batched=True)\n",
    "processed_dataset_validation = dataset_validation.map(preprocess_func, batched=True)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T13:28:30.898694Z",
     "iopub.execute_input": "2024-06-18T13:28:30.899135Z",
     "iopub.status.idle": "2024-06-18T13:28:31.108427Z",
     "shell.execute_reply.started": "2024-06-18T13:28:30.899099Z",
     "shell.execute_reply": "2024-06-18T13:28:31.107348Z"
    },
    "trusted": true
   },
   "execution_count": 135,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Map:   0%|          | 0/2 [00:00<?, ? examples/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "89515aaff252409ca31d7e5b2e9cba46"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install evaluate"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T13:24:37.219839Z",
     "iopub.execute_input": "2024-06-18T13:24:37.220319Z",
     "iopub.status.idle": "2024-06-18T13:24:52.705137Z",
     "shell.execute_reply.started": "2024-06-18T13:24:37.220284Z",
     "shell.execute_reply": "2024-06-18T13:24:52.703984Z"
    },
    "trusted": true
   },
   "execution_count": 121,
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: datasets>=2.0.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.19.2)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.2.1)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.10/site-packages (from evaluate) (4.66.4)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from evaluate) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.3.1)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /opt/conda/lib/python3.10/site-packages (from evaluate) (0.23.2)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from evaluate) (21.3)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\nRequirement already satisfied: pyarrow>=12.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (14.0.2)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (0.6)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (3.9.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from datasets>=2.0.0->evaluate) (6.0.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging->evaluate) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests>=2.19.0->evaluate) (2024.2.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->evaluate) (2023.4)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.3)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\nDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001B[2K   \u001B[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001B[0m \u001B[32m84.1/84.1 kB\u001B[0m \u001B[31m1.9 MB/s\u001B[0m eta \u001B[36m0:00:00\u001B[0ma \u001B[36m0:00:01\u001B[0m\n\u001B[?25hInstalling collected packages: evaluate\nSuccessfully installed evaluate-0.4.2\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "accuracy=evaluate.load(\"accuracy\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels=p\n",
    "    predictions=np.argmax(predictions, axis=1)\n",
    "    \n",
    "    return {\"accuracy\": accuracy.compute(predictions=predictions, references=labels)}"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T13:24:54.919241Z",
     "iopub.execute_input": "2024-06-18T13:24:54.920164Z",
     "iopub.status.idle": "2024-06-18T13:24:55.514573Z",
     "shell.execute_reply.started": "2024-06-18T13:24:54.920127Z",
     "shell.execute_reply": "2024-06-18T13:24:55.513368Z"
    },
    "trusted": true
   },
   "execution_count": 122,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3dd2169b3f3d4e299b682fe31558a206"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "# ignore tokenizer pad token in the loss\n",
    "label_pad_token_id=-100\n",
    "\n",
    "# padding the sentence of the entire datasets\n",
    "data_collator=DataCollatorForSeq2Seq(\n",
    "    tokenizer=tokenizer, \n",
    "    model=peft_model,\n",
    "    label_pad_token_id=label_pad_token_id,\n",
    "    pad_to_multiple_of=8\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T13:32:05.008843Z",
     "iopub.execute_input": "2024-06-18T13:32:05.009285Z",
     "iopub.status.idle": "2024-06-18T13:32:05.015417Z",
     "shell.execute_reply.started": "2024-06-18T13:32:05.009221Z",
     "shell.execute_reply": "2024-06-18T13:32:05.014228Z"
    },
    "trusted": true
   },
   "execution_count": 148,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!mkdir \"logs\""
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T13:26:17.890351Z",
     "iopub.execute_input": "2024-06-18T13:26:17.891278Z",
     "iopub.status.idle": "2024-06-18T13:26:19.110495Z",
     "shell.execute_reply.started": "2024-06-18T13:26:17.891227Z",
     "shell.execute_reply": "2024-06-18T13:26:19.108829Z"
    },
    "trusted": true
   },
   "execution_count": 128,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import Seq2SeqTrainer, Seq2SeqTrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "training_args=Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./output\",\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-3,\n",
    "    num_train_epochs=5,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=1,\n",
    "    logging_dir=\"logs\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    logging_steps=5,\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"wandb\",\n",
    ")\n",
    "\n",
    "\n",
    "# Create Trainer instance\n",
    "trainer=Seq2SeqTrainer(\n",
    "    model=peft_model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=processed_dataset_train,\n",
    "    eval_dataset=processed_dataset_test,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "peft_model.config.use_cache=False\n",
    "trainer.train()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T13:32:08.247299Z",
     "iopub.execute_input": "2024-06-18T13:32:08.247708Z",
     "iopub.status.idle": "2024-06-18T13:32:08.468219Z",
     "shell.execute_reply.started": "2024-06-18T13:32:08.247674Z",
     "shell.execute_reply": "2024-06-18T13:32:08.466544Z"
    },
    "trusted": true
   },
   "execution_count": 149,
   "outputs": [
    {
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[149], line 20\u001B[0m\n\u001B[1;32m      4\u001B[0m training_args\u001B[38;5;241m=\u001B[39mSeq2SeqTrainingArguments(\n\u001B[1;32m      5\u001B[0m     output_dir\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m./output\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m      6\u001B[0m     auto_find_batch_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     15\u001B[0m     report_to\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwandb\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     16\u001B[0m )\n\u001B[1;32m     19\u001B[0m \u001B[38;5;66;03m# Create Trainer instance\u001B[39;00m\n\u001B[0;32m---> 20\u001B[0m trainer\u001B[38;5;241m=\u001B[39m\u001B[43mSeq2SeqTrainer\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m     21\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpeft_model\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     22\u001B[0m \u001B[43m    \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtraining_args\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     23\u001B[0m \u001B[43m    \u001B[49m\u001B[43mdata_collator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_collator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     24\u001B[0m \u001B[43m    \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprocessed_dataset_train\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     25\u001B[0m \u001B[43m    \u001B[49m\u001B[43meval_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mprocessed_dataset_test\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     26\u001B[0m \u001B[43m    \u001B[49m\u001B[43mcompute_metrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompute_metrics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     27\u001B[0m \u001B[43m)\u001B[49m\n\u001B[1;32m     29\u001B[0m peft_model\u001B[38;5;241m.\u001B[39mconfig\u001B[38;5;241m.\u001B[39muse_cache\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[1;32m     30\u001B[0m trainer\u001B[38;5;241m.\u001B[39mtrain()\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer_seq2seq.py:57\u001B[0m, in \u001B[0;36mSeq2SeqTrainer.__init__\u001B[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001B[0m\n\u001B[1;32m     43\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__init__\u001B[39m(\n\u001B[1;32m     44\u001B[0m     \u001B[38;5;28mself\u001B[39m,\n\u001B[1;32m     45\u001B[0m     model: Union[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPreTrainedModel\u001B[39m\u001B[38;5;124m\"\u001B[39m, nn\u001B[38;5;241m.\u001B[39mModule] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     55\u001B[0m     preprocess_logits_for_metrics: Optional[Callable[[torch\u001B[38;5;241m.\u001B[39mTensor, torch\u001B[38;5;241m.\u001B[39mTensor], torch\u001B[38;5;241m.\u001B[39mTensor]] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[1;32m     56\u001B[0m ):\n\u001B[0;32m---> 57\u001B[0m     \u001B[38;5;28;43msuper\u001B[39;49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[38;5;21;43m__init__\u001B[39;49m\u001B[43m(\u001B[49m\n\u001B[1;32m     58\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     59\u001B[0m \u001B[43m        \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     60\u001B[0m \u001B[43m        \u001B[49m\u001B[43mdata_collator\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mdata_collator\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     61\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtrain_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtrain_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     62\u001B[0m \u001B[43m        \u001B[49m\u001B[43meval_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meval_dataset\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     63\u001B[0m \u001B[43m        \u001B[49m\u001B[43mtokenizer\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mtokenizer\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     64\u001B[0m \u001B[43m        \u001B[49m\u001B[43mmodel_init\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmodel_init\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     65\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcompute_metrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcompute_metrics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     66\u001B[0m \u001B[43m        \u001B[49m\u001B[43mcallbacks\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mcallbacks\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     67\u001B[0m \u001B[43m        \u001B[49m\u001B[43moptimizers\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43moptimizers\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     68\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpreprocess_logits_for_metrics\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mpreprocess_logits_for_metrics\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m     69\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m     71\u001B[0m     \u001B[38;5;66;03m# Override self.model.generation_config if a GenerationConfig is specified in args.\u001B[39;00m\n\u001B[1;32m     72\u001B[0m     \u001B[38;5;66;03m# Priority: args.generation_config > model.generation_config > default GenerationConfig.\u001B[39;00m\n\u001B[1;32m     73\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39margs\u001B[38;5;241m.\u001B[39mgeneration_config \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n",
      "File \u001B[0;32m/opt/conda/lib/python3.10/site-packages/transformers/trainer.py:475\u001B[0m, in \u001B[0;36mTrainer.__init__\u001B[0;34m(self, model, args, data_collator, train_dataset, eval_dataset, tokenizer, model_init, compute_metrics, callbacks, optimizers, preprocess_logits_for_metrics)\u001B[0m\n\u001B[1;32m    473\u001B[0m \u001B[38;5;66;03m# At this stage the model is already loaded\u001B[39;00m\n\u001B[1;32m    474\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m _is_quantized_and_base_model \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _is_peft_model(model):\n\u001B[0;32m--> 475\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    476\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mYou cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    477\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    478\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m for more details\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    479\u001B[0m     )\n\u001B[1;32m    480\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m _is_quantized_and_base_model \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m _quantization_method_supports_training:\n\u001B[1;32m    481\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[1;32m    482\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mThe model you are trying to fine-tune is quantized with \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;241m.\u001B[39mhf_quantizer\u001B[38;5;241m.\u001B[39mquantization_config\u001B[38;5;241m.\u001B[39mquant_method\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    483\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m but that quantization method do not support training. Please open an issue on GitHub: https://github.com/huggingface/transformers\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    484\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m to request the support for training support for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mmodel\u001B[38;5;241m.\u001B[39mhf_quantizer\u001B[38;5;241m.\u001B[39mquantization_config\u001B[38;5;241m.\u001B[39mquant_method\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    485\u001B[0m     )\n",
      "\u001B[0;31mValueError\u001B[0m: You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft for more details"
     ],
     "ename": "ValueError",
     "evalue": "You cannot perform fine-tuning on purely quantized models. Please attach trainable adapters on top of the quantized model to correctly perform fine-tuning. Please see: https://huggingface.co/docs/transformers/peft for more details",
     "output_type": "error"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def collate_fn(batch):\n",
    "    input_ids = torch.stack([torch.tensor(example['input_ids']) for example in batch])\n",
    "    attention_mask = torch.stack([torch.tensor(example['attention_mask']) for example in batch])\n",
    "    labels = torch.stack([torch.tensor(example['labels']) for example in batch])\n",
    "    return {'input_ids': input_ids, 'attention_mask': attention_mask, 'labels': labels}\n",
    "\n",
    "\n",
    "batch_size = 12\n",
    "dataloader = DataLoader(processed_dataset, batch_size=batch_size, collate_fn=collate_fn)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T13:12:05.866011Z",
     "iopub.execute_input": "2024-06-18T13:12:05.866352Z",
     "iopub.status.idle": "2024-06-18T13:12:05.873669Z",
     "shell.execute_reply.started": "2024-06-18T13:12:05.866322Z",
     "shell.execute_reply": "2024-06-18T13:12:05.872548Z"
    },
    "trusted": true
   },
   "execution_count": 108,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def evaluate_model(model, dataloader, metric, max_batch = 10):\n",
    "    model.eval()\n",
    "    translations = []\n",
    "    references = []\n",
    "    i = 0\n",
    "    for batch in tqdm(dataloader):\n",
    "        if i >= max_batch: break\n",
    "        input_ids = batch['input_ids'].to(model.device)\n",
    "        attention_mask = batch['attention_mask'].to(model.device)\n",
    "        labels = batch['labels'].to(model.device)\n",
    "        \n",
    "        # Generate translation\n",
    "        with torch.no_grad():\n",
    "            output_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=128, early_stopping=True)\n",
    "        \n",
    "        # Decode the generated text\n",
    "        generated_texts = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n",
    "        reference_texts = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "        \n",
    "        translations.extend(generated_texts)\n",
    "        references.extend([[ref] for ref in reference_texts])\n",
    "        i += 1\n",
    "    # Compute the BLEU score\n",
    "    results = metric.compute(predictions=translations, references=references)\n",
    "    return results\n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_model(model, dataloader, bleu)\n",
    "print(\"BLEU score:\", results[\"score\"])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T13:16:45.467644Z",
     "iopub.execute_input": "2024-06-18T13:16:45.468465Z",
     "iopub.status.idle": "2024-06-18T13:20:29.778672Z",
     "shell.execute_reply.started": "2024-06-18T13:16:45.468427Z",
     "shell.execute_reply": "2024-06-18T13:20:29.777553Z"
    },
    "trusted": true
   },
   "execution_count": 110,
   "outputs": [
    {
     "name": "stderr",
     "text": "  4%|▍         | 10/251 [03:44<1:30:01, 22.41s/it]",
     "output_type": "stream"
    },
    {
     "name": "stdout",
     "text": "BLEU score: 0.11943493356414839\n",
     "output_type": "stream"
    },
    {
     "name": "stderr",
     "text": "\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import gc\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-18T12:28:57.887833Z",
     "iopub.execute_input": "2024-06-18T12:28:57.888509Z",
     "iopub.status.idle": "2024-06-18T12:28:58.617519Z",
     "shell.execute_reply.started": "2024-06-18T12:28:57.888474Z",
     "shell.execute_reply": "2024-06-18T12:28:58.616521Z"
    },
    "trusted": true
   },
   "execution_count": 73,
   "outputs": [
    {
     "execution_count": 73,
     "output_type": "execute_result",
     "data": {
      "text/plain": "0"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {},
   "execution_count": null,
   "outputs": []
  }
 ]
}

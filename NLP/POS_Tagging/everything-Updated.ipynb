{
 "metadata": {
  "deepnote_notebook_id": "a458e12a9e1b47c689218e6a20bcfcb0",
  "deepnote_execution_queue": [],
  "colab": {
   "provenance": []
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3",
   "language": "python"
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [],
   "dockerImageVersionId": 30733,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Example: POS Tagging\n",
    "\n",
    "According to [Wikipedia](https://en.wikipedia.org/wiki/Part-of-speech_tagging):\n",
    "\n",
    "> Part-of-speech tagging (POS tagging or PoS tagging or POST) is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech, based on both its definition and its contextâ€”i.e., its relationship with adjacent and related words in a phrase, sentence, or paragraph.\n",
    "\n",
    "Formally, given a sequence of words $\\mathbf{x} = \\left< x_1, x_2, \\ldots, x_t \\right>$ the goal is to learn a model $P(y_i \\,|\\, \\mathbf{x})$ where $y_i$ is the POS tag associated with the $x_i$.\n",
    "Note that the model is conditioned on all of $\\mathbf{x}$ not just the words that occur earlier in the sentence - this is because we can assume that the entire sentence is known at the time of tagging.\n",
    "\n",
    "### Dataset\n",
    "\n",
    "We will train our model on the [Engligh Dependencies Treebank](https://github.com/UniversalDependencies/UD_English).\n",
    "You can download this dataset by running the following lines:"
   ],
   "metadata": {
    "cell_id": "81e027952c094ce1b1742a1222b82d77",
    "deepnote_cell_type": "markdown",
    "id": "2SrkISjXoKK2"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install gdown"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-15T16:17:30.358100Z",
     "iopub.execute_input": "2024-06-15T16:17:30.358451Z",
     "iopub.status.idle": "2024-06-15T16:17:44.603078Z",
     "shell.execute_reply.started": "2024-06-15T16:17:30.358423Z",
     "shell.execute_reply": "2024-06-15T16:17:44.601850Z"
    },
    "trusted": true
   },
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting gdown\n  Downloading gdown-5.2.0-py3-none-any.whl.metadata (5.8 kB)\nRequirement already satisfied: beautifulsoup4 in /opt/conda/lib/python3.10/site-packages (from gdown) (4.12.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from gdown) (3.13.1)\nRequirement already satisfied: requests[socks] in /opt/conda/lib/python3.10/site-packages (from gdown) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from gdown) (4.66.4)\nRequirement already satisfied: soupsieve>1.2 in /opt/conda/lib/python3.10/site-packages (from beautifulsoup4->gdown) (2.5)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (2024.2.2)\nRequirement already satisfied: PySocks!=1.5.7,>=1.5.6 in /opt/conda/lib/python3.10/site-packages (from requests[socks]->gdown) (1.7.1)\nDownloading gdown-5.2.0-py3-none-any.whl (18 kB)\nInstalling collected packages: gdown\nSuccessfully installed gdown-5.2.0\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import gdown\n",
    "url = \"https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-dev.conllu\"\n",
    "output = \"en_ewt-ud-dev.conllu\"\n",
    "gdown.download(url, output, quiet=False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "UwlQccQetgMq",
    "outputId": "502155bf-1d1b-4618-b2a0-c59b8b6fc1ed",
    "execution": {
     "iopub.status.busy": "2024-06-15T16:17:44.605066Z",
     "iopub.execute_input": "2024-06-15T16:17:44.605377Z",
     "iopub.status.idle": "2024-06-15T16:17:45.366360Z",
     "shell.execute_reply.started": "2024-06-15T16:17:44.605348Z",
     "shell.execute_reply": "2024-06-15T16:17:45.365438Z"
    },
    "trusted": true
   },
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "text": "Downloading...\nFrom: https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-dev.conllu\nTo: /kaggle/working/en_ewt-ud-dev.conllu\n1.76MB [00:00, 114MB/s]                   \n",
     "output_type": "stream"
    },
    {
     "execution_count": 2,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'en_ewt-ud-dev.conllu'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "url = \"https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-test.conllu\"\n",
    "output = \"en_ewt-ud-test.conllu\"\n",
    "gdown.download(url, output, quiet=False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "uQimwagRw1EI",
    "outputId": "eb6f90af-d1c7-4247-b531-a03c0a09a25a",
    "execution": {
     "iopub.status.busy": "2024-06-15T16:17:49.296135Z",
     "iopub.execute_input": "2024-06-15T16:17:49.296489Z",
     "iopub.status.idle": "2024-06-15T16:17:49.335238Z",
     "shell.execute_reply.started": "2024-06-15T16:17:49.296461Z",
     "shell.execute_reply": "2024-06-15T16:17:49.334223Z"
    },
    "trusted": true
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "text": "Downloading...\nFrom: https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-test.conllu\nTo: /kaggle/working/en_ewt-ud-test.conllu\n1.77MB [00:00, 117MB/s]                   \n",
     "output_type": "stream"
    },
    {
     "execution_count": 4,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'en_ewt-ud-test.conllu'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "url = \"https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu\"\n",
    "output = \"en_ewt-ud-train.conllu\"\n",
    "gdown.download(url, output, quiet=False)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "id": "ddJ8LKLIw-Rn",
    "outputId": "a598b0ff-67d1-4340-ca4c-187a680bf36d",
    "execution": {
     "iopub.status.busy": "2024-06-15T16:17:49.584124Z",
     "iopub.execute_input": "2024-06-15T16:17:49.584474Z",
     "iopub.status.idle": "2024-06-15T16:17:51.628351Z",
     "shell.execute_reply.started": "2024-06-15T16:17:49.584447Z",
     "shell.execute_reply": "2024-06-15T16:17:51.627467Z"
    },
    "trusted": true
   },
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "text": "Downloading...\nFrom: https://raw.githubusercontent.com/UniversalDependencies/UD_English-EWT/master/en_ewt-ud-train.conllu\nTo: /kaggle/working/en_ewt-ud-train.conllu\n13.9MB [00:00, 163MB/s]                    \n",
     "output_type": "stream"
    },
    {
     "execution_count": 5,
     "output_type": "execute_result",
     "data": {
      "text/plain": "'en_ewt-ud-train.conllu'"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The individual data instances come in chunks seperated by blank lines. Each chunk consists of a few starting comments, and then lines of tab-seperated fields. The fields we are interested in are the 1st and 3rd, which contain the tokenized word and POS tag respectively. An example chunk is shown below:\n",
    "\n",
    "```\n",
    "# sent_id = answers-20111107193044AAvUYBv_ans-0023\n",
    "# text = Hope you have a crapload of fun!\n",
    "1\tHope\thope\tVERB\tVBP\tMood=Ind|Tense=Pres|VerbForm=Fin\t0\troot\t0:root\t_\n",
    "2\tyou\tyou\tPRON\tPRP\tCase=Nom|Person=2|PronType=Prs\t3\tnsubj\t3:nsubj\t_\n",
    "3\thave\thave\tVERB\tVBP\tMood=Ind|Tense=Pres|VerbForm=Fin\t1\tccomp\t1:ccomp\t_\n",
    "4\ta\ta\tDET\tDT\tDefinite=Ind|PronType=Art\t5\tdet\t5:det\t_\n",
    "5\tcrapload\tcrapload\tNOUN\tNN\tNumber=Sing\t3\tobj\t3:obj\t_\n",
    "6\tof\tof\tADP\tIN\t_\t7\tcase\t7:case\t_\n",
    "7\tfun\tfun\tNOUN\tNN\tNumber=Sing\t5\tnmod\t5:nmod\tSpaceAfter=No\n",
    "8\t!\t!\tPUNCT\t.\t_\t1\tpunct\t1:punct\t_\n",
    "\n",
    "```"
   ],
   "metadata": {
    "cell_id": "5c715f5de80e4dd6bc04fff44ffb8ff8",
    "deepnote_cell_type": "markdown",
    "id": "TTl_3ZhfoKK6"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "As with most real world data, we are going to need to do some preprocessing before we can use it. The first thing we are going to need is a `Vocabulary` to map words/POS tags to integer ids. Here is a more full-featured implementation than what we used in the first tutorial:"
   ],
   "metadata": {
    "cell_id": "8889552cda3e4df58ed9302032585655",
    "deepnote_cell_type": "markdown",
    "id": "vBxR-xVLoKK6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from collections import Counter\n",
    "\n",
    "\n",
    "class Vocab(object):\n",
    "    def __init__(self, iter, max_size=None, sos_token=None, eos_token=None, unk_token=None):\n",
    "        \"\"\"Initialize the vocabulary.\n",
    "        Args:\n",
    "            iter: An iterable which produces sequences of tokens used to update\n",
    "                the vocabulary.\n",
    "            max_size: (Optional) Maximum number of tokens in the vocabulary.\n",
    "            sos_token: (Optional) Token denoting the start of a sequence.\n",
    "            eos_token: (Optional) Token denoting the end of a sequence.\n",
    "            unk_token: (Optional) Token denoting an unknown element in a\n",
    "                sequence.\n",
    "        \"\"\"\n",
    "        self.max_size = max_size\n",
    "        self.pad_token = '<pad>'\n",
    "        self.sos_token = sos_token\n",
    "        self.eos_token = eos_token\n",
    "        self.unk_token = unk_token\n",
    "\n",
    "        # Add special tokens.\n",
    "        id2word = [self.pad_token]\n",
    "        if sos_token is not None:\n",
    "            id2word.append(self.sos_token)\n",
    "        if eos_token is not None:\n",
    "            id2word.append(self.eos_token)\n",
    "        if unk_token is not None:\n",
    "            id2word.append(self.unk_token)\n",
    "\n",
    "        # Update counter with token counts.\n",
    "        counter = Counter()\n",
    "        for x in iter:\n",
    "            counter.update(x)\n",
    "\n",
    "        # Extract lookup tables.\n",
    "        if max_size is not None:\n",
    "            counts = counter.most_common(max_size)\n",
    "        else:\n",
    "            counts = counter.items()\n",
    "            counts = sorted(counts, key=lambda x: x[1], reverse=True)\n",
    "        words = [x[0] for x in counts]\n",
    "        id2word.extend(words)\n",
    "        word2id = {x: i for i, x in enumerate(id2word)}\n",
    "\n",
    "        self._id2word = id2word\n",
    "        self._word2id = word2id\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self._id2word)\n",
    "\n",
    "    def word2id(self, word):\n",
    "        \"\"\"Map a word in the vocabulary to its unique integer id.\n",
    "        Args:\n",
    "            word: Word to lookup.\n",
    "        Returns:\n",
    "            id: The integer id of the word being looked up.\n",
    "        \"\"\"\n",
    "        if word in self._word2id:\n",
    "            return self._word2id[word]\n",
    "        elif self.unk_token is not None:\n",
    "            return self._word2id[self.unk_token]\n",
    "        else:\n",
    "            raise KeyError('Word \"%s\" not in vocabulary.' % word)\n",
    "\n",
    "    def id2word(self, id):\n",
    "        \"\"\"Map an integer id to its corresponding word in the vocabulary.\n",
    "        Args:\n",
    "            id: Integer id of the word being looked up.\n",
    "        Returns:\n",
    "            word: The corresponding word.\n",
    "        \"\"\"\n",
    "        return self._id2word[id]"
   ],
   "metadata": {
    "cell_id": "717aa9219d174df8b1b60e34916abc78",
    "deepnote_cell_type": "code",
    "id": "aOkJywlDoKK6",
    "execution": {
     "iopub.status.busy": "2024-06-15T16:17:51.630593Z",
     "iopub.execute_input": "2024-06-15T16:17:51.630962Z",
     "iopub.status.idle": "2024-06-15T16:17:51.643476Z",
     "shell.execute_reply.started": "2024-06-15T16:17:51.630931Z",
     "shell.execute_reply": "2024-06-15T16:17:51.642601Z"
    },
    "trusted": true
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we need to parse the .conllu files and extract the data needed for our model. The good news is that the file is only a few megabytes so we can store everything in memory. Rather than creating a generator from scratch like we did in the previous tutorial, we will instead showcase the `torch.utils.data.Dataset` class. There are two main things that a `Dataset` must have:\n",
    "\n",
    "1. A `__len__` method which let's you know how many data points are in the dataset.\n",
    "2. A `__getitem__` method which is used to support integer indexing.\n",
    "\n",
    "Here's an example of how to define these methods for the English Dependencies Treebank data."
   ],
   "metadata": {
    "cell_id": "d23e9223e56c48128ccc2da382f3c39e",
    "deepnote_cell_type": "markdown",
    "id": "Tqr8Hw21oKK7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import re\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class Annotation(object):\n",
    "    def __init__(self):\n",
    "        \"\"\"A helper object for storing annotation data.\"\"\"\n",
    "        self.tokens = []\n",
    "        self.pos_tags = []\n",
    "\n",
    "\n",
    "class CoNLLDataset(Dataset):\n",
    "    def __init__(self, fname):\n",
    "        \"\"\"Initializes the CoNLLDataset.\n",
    "        Args:\n",
    "            fname: The .conllu file to load data from.\n",
    "        \"\"\"\n",
    "        self.fname = fname\n",
    "        self.annotations = self.process_conll_file(fname)\n",
    "        self.token_vocab = Vocab([x.tokens for x in self.annotations],\n",
    "                                 unk_token='<unk>')\n",
    "        self.pos_vocab = Vocab([x.pos_tags for x in self.annotations])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        annotation = self.annotations[idx]\n",
    "        input = [self.token_vocab.word2id(x) for x in annotation.tokens]\n",
    "        target = [self.pos_vocab.word2id(x) for x in annotation.pos_tags]\n",
    "        return input, target\n",
    "\n",
    "    def process_conll_file(self, fname):\n",
    "        # Read the entire file.\n",
    "        with open(fname, 'r') as f:\n",
    "            raw_text = f.read()\n",
    "        # Split into chunks on blank lines.\n",
    "        chunks = re.split(r'^\\n', raw_text, flags=re.MULTILINE)\n",
    "        # Process each chunk into an annotation.\n",
    "        annotations = []\n",
    "        for chunk in chunks:\n",
    "            annotation = Annotation()\n",
    "            lines = chunk.split('\\n')\n",
    "            # Iterate over all lines in the chunk.\n",
    "            for line in lines:\n",
    "                # If line is empty ignore it.\n",
    "                if len(line)==0:\n",
    "                    continue\n",
    "                # If line is a commend ignore it.\n",
    "                if line[0] == '#':\n",
    "                    continue\n",
    "                # Otherwise split on tabs and retrieve the token and the\n",
    "                # POS tag fields.\n",
    "                fields = line.split('\\t')\n",
    "                annotation.tokens.append(fields[1])\n",
    "                annotation.pos_tags.append(fields[3])\n",
    "            if (len(annotation.tokens) > 0) and (len(annotation.pos_tags) > 0):\n",
    "                annotations.append(annotation)\n",
    "        return annotations"
   ],
   "metadata": {
    "cell_id": "3867af02ac2644839d8919751374f0f8",
    "deepnote_cell_type": "code",
    "id": "1zcRDR7ooKK7",
    "execution": {
     "iopub.status.busy": "2024-06-15T16:17:51.644804Z",
     "iopub.execute_input": "2024-06-15T16:17:51.645140Z",
     "iopub.status.idle": "2024-06-15T16:17:54.688881Z",
     "shell.execute_reply.started": "2024-06-15T16:17:51.645110Z",
     "shell.execute_reply": "2024-06-15T16:17:54.688083Z"
    },
    "trusted": true
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "And let's see how this is used in practice."
   ],
   "metadata": {
    "cell_id": "85c38fbb082a410bb13af9edf4858c1d",
    "deepnote_cell_type": "markdown",
    "id": "TaU7d1LpoKK7"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "dataset = CoNLLDataset('en_ewt-ud-train.conllu')"
   ],
   "metadata": {
    "cell_id": "9a068884e273430c86cd54848222fac2",
    "deepnote_cell_type": "code",
    "id": "YGQ6_s49oKK7",
    "execution": {
     "iopub.status.busy": "2024-06-15T16:17:54.690666Z",
     "iopub.execute_input": "2024-06-15T16:17:54.691102Z",
     "iopub.status.idle": "2024-06-15T16:17:55.291284Z",
     "shell.execute_reply.started": "2024-06-15T16:17:54.691075Z",
     "shell.execute_reply": "2024-06-15T16:17:55.290310Z"
    },
    "trusted": true
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "input, target = dataset[0]\n",
    "print('Example input: %s\\n' % input)\n",
    "print('Example target: %s\\n' % target)\n",
    "print('Translated input: %s\\n' % ' '.join(dataset.token_vocab.id2word(x) for x in input))\n",
    "print('Translated target: %s\\n' % ' '.join(dataset.pos_vocab.id2word(x) for x in target))"
   ],
   "metadata": {
    "cell_id": "bed53de2b73d43de8965fb23d2a5a717",
    "deepnote_cell_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "LPeOna-roKK7",
    "outputId": "c38a97f4-db9f-47ce-929c-a935fd817bdf",
    "execution": {
     "iopub.status.busy": "2024-06-15T16:17:55.292554Z",
     "iopub.execute_input": "2024-06-15T16:17:55.292918Z",
     "iopub.status.idle": "2024-06-15T16:17:55.299850Z",
     "shell.execute_reply.started": "2024-06-15T16:17:55.292870Z",
     "shell.execute_reply": "2024-06-15T16:17:55.298854Z"
    },
    "trusted": true
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "text": "Example input: [266, 16, 5249, 45, 295, 703, 1154, 4233, 10099, 595, 16, 10100, 4, 3, 6865, 35, 3, 6866, 10, 3, 498, 8, 6867, 4, 758, 3, 2224, 1605, 2]\n\nExample target: [9, 2, 9, 2, 7, 1, 3, 9, 9, 9, 2, 9, 2, 6, 1, 5, 6, 1, 5, 6, 1, 5, 9, 2, 5, 6, 7, 1, 2]\n\nTranslated input: Al - Zaman : American forces killed Shaikh Abdullah al - Ani , the preacher at the mosque in the town of Qaim , near the Syrian border .\n\nTranslated target: PROPN PUNCT PROPN PUNCT ADJ NOUN VERB PROPN PROPN PROPN PUNCT PROPN PUNCT DET NOUN ADP DET NOUN ADP DET NOUN ADP PROPN PUNCT ADP DET ADJ NOUN PUNCT\n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "The main upshot of using the `Dataset` class is that it makes accessing training/test observations very simple. Accordingly, this makes batch generation easy since all we need to do is randomly choose numbers and then grab those observations from the dataset - PyTorch includes a `torch.utils.data.DataLoader` object which handles this for you. In fact, if we were not working with sequential data we would be able to proceed straight to the modeling step from here. However, since we are working with sequential data there is one last pesky issue we need to handle - padding.\n",
    "\n",
    "The issue is that when we are given a batch of outputs from `CoNLLDataset`, the sequences in the batch are likely to all be of different length. To deal with this, we define a custom `collate_annotations` function which adds padding to the end of the sequences in the batch so that they are all the same length. In addition, we'll have this function take care of loading the data into tensors and ensuring that the tensor dimensions are in the order expected by PyTorch.\n",
    "\n",
    "Oh and one last annoying thing - to deal with some of the issues caused by using padded data we will be using a function called `torch.nn.utils.rnn.pack_padded_sequences` in our model later on. All you need to know now is that this function expects our sequences in the batch to be sorted in terms of descending length, and that we know the lengths of each sequence. So we will make sure that the `collate_annotations` function performs this sorting for us and returns the sequence lengths in addition to the input and target tensors."
   ],
   "metadata": {
    "cell_id": "1cc73f92ecd64f0289e61aeb8077979b",
    "deepnote_cell_type": "markdown",
    "id": "Ycbj4TF6oKK8"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "def pad(sequences, max_length, pad_value=0):\n",
    "    \"\"\"Pads a list of sequences.\n",
    "    Args:\n",
    "        sequences: A list of sequences to be padded.\n",
    "        max_length: The length to pad to.\n",
    "        pad_value: The value used for padding.\n",
    "    Returns:\n",
    "        A list of padded sequences.\n",
    "    \"\"\"\n",
    "    out = []\n",
    "    for sequence in sequences:\n",
    "        padded = sequence + [0]*(max_length - len(sequence))\n",
    "        out.append(padded)\n",
    "    return out\n",
    "\n",
    "\n",
    "def collate_annotations(batch):\n",
    "    \"\"\"Function used to collate data returned by CoNLLDataset.\"\"\"\n",
    "    # Get inputs, targets, and lengths.\n",
    "    inputs, targets = zip(*batch)\n",
    "    lengths = [len(x) for x in inputs]\n",
    "    # Sort by length.\n",
    "    sort = sorted(zip(inputs, targets, lengths),\n",
    "                  key=lambda x: x[2],\n",
    "                  reverse=True)\n",
    "    inputs, targets, lengths = zip(*sort)\n",
    "    # Pad.\n",
    "    max_length = max(lengths)\n",
    "    inputs = pad(inputs, max_length)\n",
    "    targets = pad(targets, max_length)\n",
    "    # Transpose.\n",
    "    inputs = list(map(list, zip(*inputs)))\n",
    "    targets = list(map(list, zip(*targets)))\n",
    "    # Convert to PyTorch variables.\n",
    "    inputs = Variable(torch.LongTensor(inputs))\n",
    "    targets = Variable(torch.LongTensor(targets))\n",
    "    lengths = Variable(torch.LongTensor(lengths))\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = inputs.cuda()\n",
    "        targets = targets.cuda()\n",
    "        lengths = lengths.cuda()\n",
    "    return inputs, targets, lengths"
   ],
   "metadata": {
    "cell_id": "8f553fd12b5545428c86fd7e29b476cb",
    "deepnote_cell_type": "code",
    "id": "XPBu1WeBoKK8",
    "execution": {
     "iopub.status.busy": "2024-06-15T16:17:55.301217Z",
     "iopub.execute_input": "2024-06-15T16:17:55.301834Z",
     "iopub.status.idle": "2024-06-15T16:17:55.313713Z",
     "shell.execute_reply.started": "2024-06-15T16:17:55.301801Z",
     "shell.execute_reply": "2024-06-15T16:17:55.312719Z"
    },
    "trusted": true
   },
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Again let's see how this is used in practice:"
   ],
   "metadata": {
    "cell_id": "f7ec9ddc62364b94b6d67469e007f3cf",
    "deepnote_cell_type": "markdown",
    "id": "VOYmULYaoKK9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "for inputs, targets, lengths in DataLoader(dataset, batch_size=16, collate_fn=collate_annotations):\n",
    "    print('Inputs: %s\\n' % inputs.data)\n",
    "    print('Targets: %s\\n' % targets.data)\n",
    "    print('Lengths: %s\\n' % lengths.data)\n",
    "\n",
    "    # Usually we'd keep sampling batches, but here we'll just break\n",
    "    break"
   ],
   "metadata": {
    "scrolled": true,
    "cell_id": "e202cebd5a3247e9876364f931771ad1",
    "deepnote_cell_type": "code",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m7REdnPCoKK9",
    "outputId": "59fd325f-47d6-480e-dd6c-1c611bbf0ca6",
    "execution": {
     "iopub.status.busy": "2024-06-15T16:17:55.314994Z",
     "iopub.execute_input": "2024-06-15T16:17:55.315284Z",
     "iopub.status.idle": "2024-06-15T16:17:55.545994Z",
     "shell.execute_reply.started": "2024-06-15T16:17:55.315261Z",
     "shell.execute_reply": "2024-06-15T16:17:55.545072Z"
    },
    "trusted": true
   },
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "text": "Inputs: tensor([[   28,  1083,   266,    28,    30,   106,    68,   266,   499,   625,\n         10103,   121,  1212,    28,    28,   108],\n        [10106,     3,    16,  1713,  6874,  6878, 10115,    16,  1030,   106,\n            45, 10123,     8,  3581,  1081,  1606],\n        [   10,  5252,  5249,  4237,    11,    11,    46,  5249,  4239,  1712,\n           555,     4,    69,    60,    19,    54],\n        [  180,    19,    45,     8,    10,     3,   185,    45,    51,     8,\n          1849,  6874,    60,  1370,   159,    41],\n        [   11,   343,   295, 10118, 10125,   759,   138,  5253, 10121,     7,\n          2018,  3111,   159,    10,   450,    19],\n        [ 4234,   163,   703,  3111,   180,  1031,     8,  1154,     7, 10101,\n            12,     4,   450,     3,    44, 10111],\n        [    5,     5,  1154,  2018,     6,    10,     3,     7, 10122, 10102,\n            31,   151,    44, 10112,     3,     3],\n        [    3,   408,  4233,    12,    50,     3,  2755,   807,  3112,    32,\n            51,   219,   144,     6,   607,   582],\n        [  142,  1470, 10099,  2756,    52,   207,  1851,     8,     6,    22,\n         10104,  1714,   704,   595,     8,    21],\n        [ 1029,    10,   595,    51,  6875,     8,    72,     3,    66,  3580,\n            61,    60,     8,    16,    52,    66],\n        [    4,  6871,    16,  4238,  2225,  2756,    60, 10116,   738,   140,\n           172,  2469,     3,  1290,  2019,  3110],\n        [   57,     6, 10100,  1607,   905,    10,  6873,  5254,   231,  1469,\n          1080,   136,  5250, 10113,  1371,  2020],\n        [   25,  4235,     4,  2021,     4,     3,   320,  1852,    31,    14,\n          3581,    31,     8,     8,     2,     2],\n        [   48,    61,     3,    10,    72,  1372,    14,   132,    60,   154,\n          1370,    84,     3,     3,     0,     0],\n        [   22,  1155,  6865,  5255,    23,    29,     3, 10117,    20,     5,\n            10,    22, 10105,  1082,     0,     0],\n        [   62,  3581,    35,     4,    20,  3113,    86,    82,    63,   216,\n          1850, 10124,    49,     2,     0,     0],\n        [  309,     4,     3, 10119,     3,     5,   660,    10,   111,     2,\n             2,     2,     0,     0,     0,     0],\n        [ 5251,     9,  6866,    58,  6876,     3,    10,  5255,   234,   626,\n             0,     0,     0,     0,     0,     0],\n        [10107,    96,    10,    51,    14,  2226,   904,     2,     2,     0,\n             0,     0,     0,     0,     0,     0],\n        [10108,    36,     3,    70,  6877,    16,     2,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [10109,    38,   498,     7, 10126,  1715,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [ 6868,   181,     8,   378,     6,     8,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [   35,     3,  6867,  2022,  5256,     3,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [    7,   705,     4,    10,   906,   321,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [  362,    12,   758,     3,    69,     2,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [  596,     3,     3,   207,    10,    27,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [  513,  6872,  2224,     8,     2,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [    8,   683,  1605, 10120,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [    3,     5,     2,     2,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [10110, 10114,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [ 6869,     3,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [  100,   555,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [   10,   409,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [    3,    78,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [ 6870,  4236,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0],\n        [    2,     2,     0,     0,     0,     0,     0,     0,     0,     0,\n             0,     0,     0,     0,     0,     0]], device='cuda:0')\n\nTargets: tensor([[ 6, 14,  9,  6,  2,  6,  4,  9,  5,  2,  9,  5, 13,  6,  6,  4],\n        [ 9,  6,  2,  1,  9,  1,  3,  2,  9,  6,  2,  9,  5,  1,  7,  3],\n        [ 5,  1,  9,  1,  8,  8,  4,  9,  1,  1,  7,  2,  4,  8,  8, 14],\n        [ 9,  8,  2,  5,  5,  6, 10,  2,  8,  5,  1,  9,  8,  3,  8,  4],\n        [ 8, 10,  7,  9,  7,  7, 10,  1,  3,  6,  3,  9,  8,  5,  3,  8],\n        [ 7,  3,  1,  9,  9,  1,  5,  3,  6,  7, 14,  2,  3,  6,  5,  3],\n        [ 5, 12,  3,  3, 11,  5,  6,  6,  7,  1,  4, 13,  5,  9,  6,  6],\n        [ 6,  3,  9, 14,  3,  6,  9,  1,  1,  8,  8,  1, 13, 11,  1,  1],\n        [ 9,  1,  9,  1,  6,  1,  1,  5, 11,  8,  3,  1,  1,  9,  5,  5],\n        [ 9, 14,  9,  8, 10,  5,  4,  6,  4,  3,  5,  8,  5,  2,  6,  4],\n        [ 2,  3,  2,  3,  7,  1,  8,  9,  1,  4, 13,  3,  6,  9,  1,  1],\n        [10, 11,  9, 13,  1,  5, 10,  7, 14,  1,  7, 14,  9,  1,  1,  1],\n        [ 4,  3,  2,  1,  2,  6,  3,  9,  4,  5,  1,  4,  5,  5,  2,  2],\n        [ 8,  5,  6,  5,  4,  1,  5, 14,  8,  1,  3,  8,  6,  6,  0,  0],\n        [ 8,  1,  1,  9,  8, 14,  6,  3,  5, 12,  5,  8,  9,  1,  0,  0],\n        [14,  1,  5,  2,  5,  3,  7,  4,  4,  3,  9,  3,  2,  2,  0,  0],\n        [ 3,  2,  6, 14,  6,  5,  1,  5,  1,  2,  2,  2,  0,  0,  0,  0],\n        [ 9,  4,  1,  4,  1,  6,  5,  9, 10,  2,  0,  0,  0,  0,  0,  0],\n        [ 9, 15,  5,  8,  5,  1,  9,  2,  2,  0,  0,  0,  0,  0,  0,  0],\n        [ 9,  8,  6,  3,  9,  2,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [10, 12,  1,  6,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 3,  3,  5,  7, 11,  5,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 5,  6,  9,  1, 10,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 6,  1,  2,  5,  3,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 7, 14,  5,  6,  4,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 1,  6,  6,  1,  5,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 1,  9,  7,  5,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 5,  3,  1,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 6, 12,  2,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 9,  3,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 1,  6,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [10,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 5,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 6, 10,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 1,  7,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0],\n        [ 2,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]],\n       device='cuda:0')\n\nLengths: tensor([36, 36, 29, 29, 27, 26, 20, 19, 19, 18, 17, 17, 16, 16, 13, 13],\n       device='cuda:0')\n\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model\n",
    "\n",
    "We will use the following architecture:\n",
    "\n",
    "1. Embed the input words into a 200 dimensional vector space.\n",
    "2. Feed the word embeddings into a (bidirectional) GRU.\n",
    "3. Feed the GRU outputs into a fully connected layer.\n",
    "4. Use a softmax activation to get the probabilities of the different labels.\n",
    "\n",
    "There is one complication which arises during the forward computation. As was noted in the dataset section, the input sequences are padded. This causes an issue since we do not want to waste computational resources feeding these pad tokens into the RNN. In PyTorch, we can deal with this issue by converting the sequence data into a  `torch.nn.utils.rnn.PackedSequence` object before feeding it into the RNN. In essence, a `PackedSequence` flattens the sequence and batch dimensions of a tensor, and also contains metadata so that PyTorch knows when to re-initialize the hidden state when fed into a recurrent layer. If this seems confusing, do not worry. To use the `PackedSequence` in practice you will almost always perform the following steps:\n",
    "\n",
    "1. Before feeding data into a recurrent layer, transform it into a `PackedSequence` by using the function `torch.nn.utils.rnn.pack_padded_sequence()`.\n",
    "2. Feed the `PackedSequence` into the recurrent layer.\n",
    "3. Transform the output back into a regular tensor by using the function `torch.nn.utils.rnn.pad_packed_sequence()`.\n",
    "\n",
    "See the model implementation below for a working example:"
   ],
   "metadata": {
    "cell_id": "17e7685b51374eb4b5d988e5e70feb4f",
    "deepnote_cell_type": "markdown",
    "id": "dvxXuiKLoKK9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from torch import nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "#RNN\n",
    "class Tagger(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_vocab_size,\n",
    "                 output_vocab_size,\n",
    "                 embedding_dim=64,\n",
    "                 hidden_size=64,\n",
    "                 bidirectional=True):\n",
    "        \"\"\"Initializes the tagger.\n",
    "\n",
    "        Args:\n",
    "            input_vocab_size: Size of the input vocabulary.\n",
    "            output_vocab_size: Size of the output vocabulary.\n",
    "            embedding_dim: Dimension of the word embeddings.\n",
    "            hidden_size: Number of units in each LSTM hidden layer.\n",
    "            bidirectional: Whether or not to use a bidirectional rnn.\n",
    "        \"\"\"\n",
    "        super(Tagger, self).__init__()\n",
    "\n",
    "        # Store parameters\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        # Define layers\n",
    "        self.word_embeddings = nn.Embedding(input_vocab_size, embedding_dim,\n",
    "                                            padding_idx=0)\n",
    "        self.rnn = nn.GRU(embedding_dim, hidden_size,\n",
    "                          bidirectional=bidirectional,\n",
    "                          dropout=0.9)\n",
    "        if bidirectional:\n",
    "            self.fc = nn.Linear(2*hidden_size, output_vocab_size)\n",
    "        else:\n",
    "            self.fc = nn.Linear(hidden_size, output_vocab_size)\n",
    "        self.activation = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, x, lengths=None, hidden=None):\n",
    "        \"\"\"Computes a forward pass of the language model.\n",
    "\n",
    "        Args:\n",
    "            x: A LongTensor w/ dimension [seq_len, batch_size].\n",
    "            lengths: The lengths of the sequences in x.\n",
    "            hidden: Hidden state to be fed into the lstm.\n",
    "\n",
    "        Returns:\n",
    "            net: the output representation for each word in the sequence.\n",
    "            hidden: the hidden state at the last timestamp.\n",
    "        \"\"\"\n",
    "        seq_len, batch_size = x.size()\n",
    "\n",
    "        # If no hidden state is provided, then default to zeros.\n",
    "        if hidden is None:\n",
    "            if self.bidirectional:\n",
    "                num_directions = 2\n",
    "            else:\n",
    "                num_directions = 1\n",
    "            hidden = Variable(torch.zeros(num_directions, batch_size, self.hidden_size))\n",
    "            if torch.cuda.is_available():\n",
    "                hidden = hidden.cuda()\n",
    "\n",
    "        net = self.word_embeddings(x)\n",
    "        # Pack before feeding into the RNN.\n",
    "        if lengths is not None:\n",
    "            lengths = lengths.data.view(-1).tolist()\n",
    "            net = pack_padded_sequence(net, lengths)\n",
    "        net, hidden = self.rnn(net, hidden)\n",
    "        # Unpack after\n",
    "        if lengths is not None:\n",
    "            net, _ = pad_packed_sequence(net)\n",
    "        net = self.fc(net)\n",
    "        net = self.activation(net)\n",
    "\n",
    "        return net, hidden"
   ],
   "metadata": {
    "cell_id": "6b0e4a55ef8e4cd187c80f26f64ccebc",
    "deepnote_cell_type": "code",
    "id": "xqI81PhsoKK9",
    "execution": {
     "iopub.status.busy": "2024-06-15T16:17:55.548809Z",
     "iopub.execute_input": "2024-06-15T16:17:55.549164Z",
     "iopub.status.idle": "2024-06-15T16:17:55.561345Z",
     "shell.execute_reply.started": "2024-06-15T16:17:55.549137Z",
     "shell.execute_reply": "2024-06-15T16:17:55.560396Z"
    },
    "trusted": true
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#CNN\n",
    "class Tagger(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_vocab_size,\n",
    "                 output_vocab_size,\n",
    "                 embedding_dim=128,\n",
    "                 num_filters=128,\n",
    "                 filter_sizes=(3, 4, 5),\n",
    "                 dropout=0.5):\n",
    "        \"\"\"Initializes the tagger.\n",
    "\n",
    "        Args:\n",
    "            input_vocab_size: Size of the input vocabulary.\n",
    "            output_vocab_size: Size of the output vocabulary.\n",
    "            embedding_dim: Dimension of the word embeddings.\n",
    "            num_filters: Number of filters for each filter size.\n",
    "            filter_sizes: Tuple of filter sizes.\n",
    "            dropout: Dropout probability.\n",
    "        \"\"\"\n",
    "        super(Tagger, self).__init__()\n",
    "\n",
    "        # Store parameters\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_filters = num_filters\n",
    "        self.filter_sizes = filter_sizes\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Define layers\n",
    "        self.word_embeddings = nn.Embedding(input_vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.convs = nn.ModuleList([\n",
    "            nn.Conv1d(embedding_dim, num_filters, filter_size)\n",
    "            for filter_size in filter_sizes\n",
    "        ])\n",
    "        self.fc1 = nn.Linear(len(filter_sizes) * num_filters, 256)\n",
    "        self.fc2 = nn.Linear(256, output_vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.activation = nn.LogSoftmax(dim=2)\n",
    "\n",
    "    def forward(self, x, lengths=None, hidden=None):\n",
    "        \"\"\"Computes a forward pass of the CNN model.\n",
    "\n",
    "        Args:\n",
    "            x: A LongTensor w/ dimension [seq_len, batch_size].\n",
    "            lengths: The lengths of the sequences in x (unused in CNN, kept for compatibility).\n",
    "            hidden: Hidden state (unused in CNN, kept for compatibility).\n",
    "\n",
    "        Returns:\n",
    "            net: the output representation for each word in the sequence.\n",
    "            hidden: None (to match the RNN output format).\n",
    "        \"\"\"\n",
    "        seq_len, batch_size = x.size()\n",
    "\n",
    "        net = self.word_embeddings(x)\n",
    "        net = net.permute(1, 2, 0)  # Permute dimensions to [batch_size, embedding_dim, seq_len]\n",
    "\n",
    "        conv_outputs = []\n",
    "        for conv in self.convs:\n",
    "            conv_out = F.relu(conv(net))\n",
    "            conv_out = F.max_pool1d(conv_out, conv_out.size(2))\n",
    "            conv_out = conv_out.squeeze(2)\n",
    "            conv_outputs.append(conv_out)\n",
    "\n",
    "        net = torch.cat(conv_outputs, 1)\n",
    "        net = self.dropout(net)\n",
    "        net = F.relu(self.fc1(net))\n",
    "        net = self.dropout(net)\n",
    "        net = self.fc2(net)\n",
    "        net = net.unsqueeze(0)  # Add sequence length dimension\n",
    "        net = net.expand(seq_len, -1, -1)  # Expand to match target shape\n",
    "\n",
    "        net = self.activation(net)\n",
    "\n",
    "        return net, None  # Return None for hidden state to match RNN output format"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-15T16:17:55.562833Z",
     "iopub.execute_input": "2024-06-15T16:17:55.563192Z",
     "iopub.status.idle": "2024-06-15T16:17:55.577354Z",
     "shell.execute_reply.started": "2024-06-15T16:17:55.563161Z",
     "shell.execute_reply": "2024-06-15T16:17:55.576600Z"
    },
    "trusted": true
   },
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install pytorch-crf"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-15T16:17:55.578581Z",
     "iopub.execute_input": "2024-06-15T16:17:55.578924Z",
     "iopub.status.idle": "2024-06-15T16:18:08.347026Z",
     "shell.execute_reply.started": "2024-06-15T16:17:55.578870Z",
     "shell.execute_reply": "2024-06-15T16:18:08.345958Z"
    },
    "trusted": true
   },
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "text": "Collecting pytorch-crf\n  Downloading pytorch_crf-0.7.2-py3-none-any.whl.metadata (2.4 kB)\nDownloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\nInstalling collected packages: pytorch-crf\nSuccessfully installed pytorch-crf-0.7.2\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchcrf\n",
    "from torchcrf import CRF\n",
    "# BiLSTM + CRF\n",
    "class Tagger(nn.Module):\n",
    "    def __init__(self,\n",
    "                 input_vocab_size,\n",
    "                 output_vocab_size,\n",
    "                 embedding_dim=128,\n",
    "                 hidden_size=256,\n",
    "                 num_layers=2,\n",
    "                 dropout=0.5,\n",
    "                 bidirectional=True):\n",
    "        \"\"\"Initializes the tagger.\n",
    "\n",
    "        Args:\n",
    "            input_vocab_size: Size of the input vocabulary.\n",
    "            output_vocab_size: Size of the output vocabulary.\n",
    "            embedding_dim: Dimension of the word embeddings.\n",
    "            hidden_size: Number of units in each LSTM hidden layer.\n",
    "            num_layers: Number of LSTM layers.\n",
    "            dropout: Dropout probability.\n",
    "            bidirectional: Whether to use a bidirectional LSTM.\n",
    "        \"\"\"\n",
    "        super(Tagger, self).__init__()\n",
    "\n",
    "        # Store parameters\n",
    "        self.input_vocab_size = input_vocab_size\n",
    "        self.output_vocab_size = output_vocab_size\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.dropout = dropout\n",
    "        self.bidirectional = bidirectional\n",
    "\n",
    "        # Define layers\n",
    "        self.word_embeddings = nn.Embedding(input_vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_size, num_layers,\n",
    "                            dropout=dropout, bidirectional=bidirectional)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(2 * hidden_size if bidirectional else hidden_size, output_vocab_size)\n",
    "        self.crf = CRF(output_vocab_size, batch_first=True)\n",
    "\n",
    "    def forward(self, x, lengths=None, hidden=None):\n",
    "        \"\"\"Computes a forward pass of the BiLSTM-CRF model.\n",
    "\n",
    "        Args:\n",
    "            x: A LongTensor w/ dimension [seq_len, batch_size].\n",
    "            lengths: The lengths of the sequences in x.\n",
    "            hidden: Hidden state to be fed into the LSTM.\n",
    "\n",
    "        Returns:\n",
    "            emissions: The emission scores for each tag.\n",
    "            hidden: The hidden state at the last timestamp.\n",
    "        \"\"\"\n",
    "        seq_len, batch_size = x.size()\n",
    "\n",
    "        # If no hidden state is provided, then default to zeros.\n",
    "        if hidden is None:\n",
    "            num_directions = 2 if self.bidirectional else 1\n",
    "            hidden = torch.zeros(self.num_layers * num_directions, batch_size, self.hidden_size)\n",
    "            if torch.cuda.is_available():\n",
    "                hidden = hidden.cuda()\n",
    "\n",
    "        # Embed the input\n",
    "        net = self.word_embeddings(x)\n",
    "\n",
    "        # Pack padded sequences and feed into LSTM\n",
    "        net = nn.utils.rnn.pack_padded_sequence(net, lengths.cpu())\n",
    "        net, hidden = self.lstm(net, (hidden, hidden))\n",
    "        net, _ = nn.utils.rnn.pad_packed_sequence(net)\n",
    "\n",
    "        # Apply dropout and feed into fully connected layer\n",
    "        net = self.dropout(net)\n",
    "        emissions = self.fc(net)\n",
    "\n",
    "        return emissions, hidden\n",
    "    \n",
    "    def decode(self, emissions, lengths):\n",
    "        \"\"\"Decodes the emission scores and returns the most likely tag sequence.\n",
    "\n",
    "        Args:\n",
    "            emissions: The emission scores for each tag.\n",
    "            lengths: The lengths of the sequences.\n",
    "\n",
    "        Returns:\n",
    "            The most likely tag sequence for each input sequence.\n",
    "        \"\"\"\n",
    "        # Transpose the emissions to match the expected shape\n",
    "        emissions = emissions.transpose(0, 1)\n",
    "\n",
    "        # Create a mask tensor based on the lengths\n",
    "        mask = torch.zeros(emissions.size()[:2], dtype=torch.bool)\n",
    "        if torch.cuda.is_available():\n",
    "            mask = mask.cuda()\n",
    "        for i, length in enumerate(lengths):\n",
    "            mask[i, :length] = True\n",
    "\n",
    "        # Ensure the mask of the first timestep is all ones\n",
    "        mask[:, 0] = True\n",
    "\n",
    "        return self.crf.decode(emissions, mask)\n",
    "\n",
    "    def loss(self, emissions, tags, lengths):\n",
    "        \"\"\"Computes the negative log-likelihood loss.\n",
    "\n",
    "        Args:\n",
    "            emissions: The emission scores for each tag.\n",
    "            tags: The true tags.\n",
    "            lengths: The lengths of the sequences.\n",
    "\n",
    "        Returns:\n",
    "            The negative log-likelihood loss.\n",
    "        \"\"\"\n",
    "        if torch.cuda.is_available():\n",
    "            emissions = emissions.cuda()\n",
    "            tags = tags.cuda()\n",
    "            lengths = lengths.cuda()\n",
    "\n",
    "        # Transpose the emissions to match the expected shape\n",
    "        emissions = emissions.transpose(0, 1)\n",
    "\n",
    "        # Create a mask tensor based on the lengths\n",
    "        mask = torch.zeros(emissions.size()[:2], dtype=torch.bool)\n",
    "        if torch.cuda.is_available():\n",
    "            mask = mask.cuda()\n",
    "        for i, length in enumerate(lengths):\n",
    "            mask[i, :length] = True\n",
    "\n",
    "        # Adjust the tags tensor to ignore padded elements\n",
    "        tags = tags.transpose(0, 1)\n",
    "        tags = tags[:, :emissions.size(1)].contiguous()\n",
    "\n",
    "        return -self.crf(emissions, tags, mask=mask)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-15T17:10:32.848602Z",
     "iopub.execute_input": "2024-06-15T17:10:32.849238Z",
     "iopub.status.idle": "2024-06-15T17:10:32.868251Z",
     "shell.execute_reply.started": "2024-06-15T17:10:32.849200Z",
     "shell.execute_reply": "2024-06-15T17:10:32.867212Z"
    },
    "trusted": true
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training\n",
    "\n",
    "Training is pretty much exactly the same as in the previous tutorial. There is one catch - we don't want to evaluate our loss function on pad tokens. This is easily fixed by setting the weight of the pad class to zero."
   ],
   "metadata": {
    "cell_id": "d890273612704134a8f8ce72b6765672",
    "deepnote_cell_type": "markdown",
    "id": "y40pNO31oKK9"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load datasets.\n",
    "train_dataset = CoNLLDataset('en_ewt-ud-train.conllu')\n",
    "dev_dataset = CoNLLDataset('en_ewt-ud-dev.conllu')\n",
    "\n",
    "dev_dataset.token_vocab = train_dataset.token_vocab\n",
    "dev_dataset.pos_vocab = train_dataset.pos_vocab\n",
    "\n",
    "# Hyperparameters / constants.\n",
    "input_vocab_size = len(train_dataset.token_vocab)\n",
    "output_vocab_size = len(train_dataset.pos_vocab)\n",
    "batch_size = 16\n",
    "epochs = 6\n",
    "\n",
    "# Initialize the model.\n",
    "model = Tagger(input_vocab_size, output_vocab_size)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# Loss function weights.\n",
    "weight = torch.ones(output_vocab_size)\n",
    "weight[0] = 0\n",
    "if torch.cuda.is_available():\n",
    "    weight = weight.cuda()\n",
    "\n",
    "# Initialize loss function and optimizer.\n",
    "loss_function = torch.nn.NLLLoss(weight)\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Main training loop.\n",
    "data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                         collate_fn=collate_annotations)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False,\n",
    "                        collate_fn=collate_annotations)\n",
    "losses = []\n",
    "i = 0\n",
    "for epoch in range(epochs):\n",
    "    for inputs, targets, lengths in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs, _ = model(inputs, lengths=lengths)\n",
    "\n",
    "        outputs = outputs.view(-1, output_vocab_size)\n",
    "        targets = targets.view(-1)\n",
    "\n",
    "        loss = loss_function(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "        if (i % 1000) == 0:\n",
    "            # Compute dev loss over entire dev set.\n",
    "            # NOTE: This is expensive. In your work you may want to only use a\n",
    "            # subset of the dev set.\n",
    "            dev_losses = []\n",
    "            for inputs, targets, lengths in dev_loader:\n",
    "                outputs, _ = model(inputs, lengths=lengths)\n",
    "                outputs = outputs.view(-1, output_vocab_size)\n",
    "                targets = targets.view(-1)\n",
    "                loss = loss_function(outputs, targets)\n",
    "                dev_losses.append(loss.item())\n",
    "            avg_train_loss = np.mean(losses)\n",
    "            avg_dev_loss = np.mean(dev_losses)\n",
    "            losses = []\n",
    "            print('Iteration %i - Train Loss: %0.6f - Dev Loss: %0.6f' % (i, avg_train_loss, avg_dev_loss))\n",
    "            torch.save(model, 'pos_tagger.pt')\n",
    "        i += 1\n",
    "\n",
    "torch.save(model, 'pos_tagger.final.pt')"
   ],
   "metadata": {
    "cell_id": "b5f27faa9d0f4d7db8314a2408fdf446",
    "deepnote_cell_type": "code",
    "id": "WL1FdHbkoKK9",
    "outputId": "3a472734-ecad-42bd-f286-e51b57d54964",
    "execution": {
     "iopub.status.busy": "2024-06-15T15:42:45.401385Z",
     "iopub.execute_input": "2024-06-15T15:42:45.401811Z",
     "iopub.status.idle": "2024-06-15T16:01:35.945476Z",
     "shell.execute_reply.started": "2024-06-15T15:42:45.401770Z",
     "shell.execute_reply": "2024-06-15T16:01:35.944418Z"
    },
    "trusted": true
   },
   "execution_count": 53,
   "outputs": [
    {
     "name": "stdout",
     "text": "Iteration 0 - Train Loss: -0.008877 - Dev Loss: -0.046099\nIteration 1000 - Train Loss: -246.874077 - Dev Loss: -483.062932\nIteration 2000 - Train Loss: -725.059759 - Dev Loss: -955.216551\nIteration 3000 - Train Loss: -1200.155136 - Dev Loss: -1425.913204\nIteration 4000 - Train Loss: -1675.247595 - Dev Loss: -1896.754818\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#BI-LSTM ONLY\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = CoNLLDataset('en_ewt-ud-train.conllu')\n",
    "dev_dataset = CoNLLDataset('en_ewt-ud-dev.conllu')\n",
    "\n",
    "dev_dataset.token_vocab = train_dataset.token_vocab\n",
    "dev_dataset.pos_vocab = train_dataset.pos_vocab\n",
    "\n",
    "# Hyperparameters and constants\n",
    "input_vocab_size = len(train_dataset.token_vocab)\n",
    "output_vocab_size = len(train_dataset.pos_vocab)\n",
    "embedding_dim = 128\n",
    "hidden_size = 256\n",
    "num_layers = 2\n",
    "dropout = 0.5\n",
    "bidirectional = True\n",
    "batch_size = 16\n",
    "epochs = 6\n",
    "\n",
    "# Initialize the model\n",
    "model = Tagger(input_vocab_size, output_vocab_size, embedding_dim, hidden_size,\n",
    "               num_layers, dropout, bidirectional)\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "\n",
    "# Initialize optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Main training loop\n",
    "data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n",
    "                         collate_fn=collate_annotations)\n",
    "dev_loader = DataLoader(dev_dataset, batch_size=batch_size, shuffle=False,\n",
    "                        collate_fn=collate_annotations)\n",
    "losses = []\n",
    "best_dev_loss = float('inf')\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    for inputs, targets, lengths in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        emissions, _ = model(inputs, lengths=lengths)\n",
    "        loss = model.loss(emissions, targets, lengths)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses.append(loss.item())\n",
    "\n",
    "    # Evaluate on dev set\n",
    "    model.eval()\n",
    "    dev_losses = []\n",
    "    with torch.no_grad():\n",
    "        # Collect the predictions and targets\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        for inputs, targets, lengths in dev_loader:\n",
    "            emissions, _ = model(inputs, lengths=lengths)\n",
    "            loss = model.loss(emissions, targets, lengths)\n",
    "            dev_losses.append(loss.item())\n",
    "            _, preds = torch.max(emissions, dim=2)\n",
    "            targets = targets.view(-1)\n",
    "            preds = preds.view(-1)\n",
    "            if torch.cuda.is_available():\n",
    "                targets = targets.cpu()\n",
    "                preds = preds.cpu()\n",
    "            y_true.append(targets.data.numpy())\n",
    "            y_pred.append(preds.data.numpy())\n",
    "    \n",
    "    avg_train_loss = np.mean(losses)\n",
    "    avg_dev_loss = np.mean(dev_losses)\n",
    "    losses = []\n",
    "    print(f'Epoch {epoch + 1} - Train Loss: {avg_train_loss:.4f} - Dev Loss: {avg_dev_loss:.4f}')\n",
    "    \n",
    "    # Stack into numpy arrays\n",
    "    y_true = np.concatenate(y_true)\n",
    "    y_pred = np.concatenate(y_pred)\n",
    "\n",
    "    # Compute accuracy\n",
    "    acc = np.mean(y_true[y_true != 0] == y_pred[y_true != 0])\n",
    "    print('Accuracy - %0.6f\\n' % acc)\n",
    "\n",
    "    # Evaluate f1-score\n",
    "    from sklearn.metrics import f1_score\n",
    "    score = f1_score(y_true, y_pred, average=None)\n",
    "    print('F1-scores:\\n')\n",
    "    for label, score in zip(dev_dataset.pos_vocab._id2word[1:], score[1:]):\n",
    "        print('%s - %0.6f' % (label, score))\n",
    "\n",
    "\n",
    "    # Save the best model based on dev loss\n",
    "    if avg_dev_loss < best_dev_loss:\n",
    "        best_dev_loss = avg_dev_loss\n",
    "        torch.save(model, 'pos_tagger_bilstm_crf.pt')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-15T17:12:04.393645Z",
     "iopub.execute_input": "2024-06-15T17:12:04.394422Z",
     "iopub.status.idle": "2024-06-15T17:16:13.770765Z",
     "shell.execute_reply.started": "2024-06-15T17:12:04.394390Z",
     "shell.execute_reply": "2024-06-15T17:16:13.769732Z"
    },
    "trusted": true
   },
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "text": "Epoch 1 - Train Loss: 190.1710 - Dev Loss: 87.0305\nAccuracy - 0.849287\n\nF1-scores:\n\nNOUN - 0.749726\nPUNCT - 0.985865\nVERB - 0.831931\nPRON - 0.975975\nADP - 0.930035\nDET - 0.979025\nADJ - 0.715971\nAUX - 0.970221\nPROPN - 0.031655\nADV - 0.798759\nCCONJ - 0.985843\nPART - 0.942474\nNUM - 0.676471\nSCONJ - 0.787293\n_ - 0.973277\nSYM - 0.700000\nINTJ - 0.616279\nX - 0.037037\nEpoch 2 - Train Loss: 81.5146 - Dev Loss: 71.1731\nAccuracy - 0.880488\n\nF1-scores:\n\nNOUN - 0.786194\nPUNCT - 0.988779\nVERB - 0.873073\nPRON - 0.982638\nADP - 0.945770\nDET - 0.982989\nADJ - 0.804380\nAUX - 0.975954\nPROPN - 0.039969\nADV - 0.851646\nCCONJ - 0.989691\nPART - 0.966102\nNUM - 0.783476\nSCONJ - 0.827404\n_ - 0.984658\nSYM - 0.719424\nINTJ - 0.673913\nX - 0.107143\nEpoch 3 - Train Loss: 50.4686 - Dev Loss: 64.7625\nAccuracy - 0.895696\n\nF1-scores:\n\nNOUN - 0.811467\nPUNCT - 0.988943\nVERB - 0.893007\nPRON - 0.982115\nADP - 0.948498\nDET - 0.984277\nADJ - 0.842989\nAUX - 0.980077\nPROPN - 0.048702\nADV - 0.869261\nCCONJ - 0.991607\nPART - 0.965517\nNUM - 0.812940\nSCONJ - 0.840206\n_ - 0.986034\nSYM - 0.753425\nINTJ - 0.713514\nX - 0.107143\nEpoch 4 - Train Loss: 32.8978 - Dev Loss: 72.1856\nAccuracy - 0.896872\n\nF1-scores:\n\nNOUN - 0.813528\nPUNCT - 0.989100\nVERB - 0.891603\nPRON - 0.983533\nADP - 0.945914\nDET - 0.984543\nADJ - 0.848229\nAUX - 0.979461\nPROPN - 0.049292\nADV - 0.866841\nCCONJ - 0.991661\nPART - 0.959937\nNUM - 0.808633\nSCONJ - 0.829530\n_ - 0.990210\nSYM - 0.741259\nINTJ - 0.713514\nX - 0.137931\nEpoch 5 - Train Loss: 21.7041 - Dev Loss: 75.7711\nAccuracy - 0.906044\n\nF1-scores:\n\nNOUN - 0.839735\nPUNCT - 0.990392\nVERB - 0.896764\nPRON - 0.984685\nADP - 0.947949\nDET - 0.983943\nADJ - 0.850465\nAUX - 0.980430\nPROPN - 0.712500\nADV - 0.879111\nCCONJ - 0.992288\nPART - 0.962441\nNUM - 0.835007\nSCONJ - 0.838292\n_ - 0.990210\nSYM - 0.750000\nINTJ - 0.717391\nX - 0.000099\nEpoch 6 - Train Loss: 15.0396 - Dev Loss: 78.5810\nAccuracy - 0.908357\n\nF1-scores:\n\nNOUN - 0.846973\nPUNCT - 0.990238\nVERB - 0.900474\nPRON - 0.984650\nADP - 0.952312\nDET - 0.985568\nADJ - 0.851873\nAUX - 0.979487\nPROPN - 0.716410\nADV - 0.877637\nCCONJ - 0.991618\nPART - 0.970359\nNUM - 0.815675\nSCONJ - 0.841146\n_ - 0.991620\nSYM - 0.719424\nINTJ - 0.730159\nX - 0.000199\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Evaluation\n",
    "\n",
    "For tagging tasks the typical evaluation metric are accuracy and f1-score (e.g. the harmonic mean of precision and recall):\n",
    "\n",
    "$$ \\text{f1-score} = 2 \\frac{\\text{precision} * \\text{recall}}{\\text{precision} + \\text{recall}} $$\n",
    "\n",
    "Here are the results for our final model:"
   ],
   "metadata": {
    "cell_id": "7203c2b15a3944099f252e6299d26fba",
    "deepnote_cell_type": "markdown",
    "id": "TJ5SiU0foKK-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# Collect the predictions and targets\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "for inputs, targets, lengths in dev_loader:\n",
    "    outputs, _ = model(inputs, lengths=lengths)\n",
    "    _, preds = torch.max(outputs, dim=2)\n",
    "    targets = targets.view(-1)\n",
    "    preds = preds.view(-1)\n",
    "    if torch.cuda.is_available():\n",
    "        targets = targets.cpu()\n",
    "        preds = preds.cpu()\n",
    "    y_true.append(targets.data.numpy())\n",
    "    y_pred.append(preds.data.numpy())\n",
    "\n",
    "# Stack into numpy arrays\n",
    "y_true = np.concatenate(y_true)\n",
    "y_pred = np.concatenate(y_pred)\n",
    "\n",
    "# Compute accuracy\n",
    "acc = np.mean(y_true[y_true != 0] == y_pred[y_true != 0])\n",
    "print('Accuracy - %0.6f\\n' % acc)\n",
    "\n",
    "# Evaluate f1-score\n",
    "from sklearn.metrics import f1_score\n",
    "score = f1_score(y_true, y_pred, average=None)\n",
    "print('F1-scores:\\n')\n",
    "for label, score in zip(dev_dataset.pos_vocab._id2word[1:], score[1:]):\n",
    "    print('%s - %0.6f' % (label, score))"
   ],
   "metadata": {
    "cell_id": "dc1ddda44d1f4771888dd4a00c4fe222",
    "deepnote_cell_type": "code",
    "id": "UK0JuMR2oKK-",
    "outputId": "896d8953-2efa-4362-e090-ae5ddabb929b",
    "execution": {
     "iopub.status.busy": "2024-06-15T17:16:13.772511Z",
     "iopub.execute_input": "2024-06-15T17:16:13.772790Z",
     "iopub.status.idle": "2024-06-15T17:16:14.207842Z",
     "shell.execute_reply.started": "2024-06-15T17:16:13.772766Z",
     "shell.execute_reply": "2024-06-15T17:16:14.206926Z"
    },
    "trusted": true
   },
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "text": "Accuracy - 0.908357\n\nF1-scores:\n\nNOUN - 0.846973\nPUNCT - 0.990238\nVERB - 0.900474\nPRON - 0.984650\nADP - 0.952312\nDET - 0.985568\nADJ - 0.851873\nAUX - 0.979487\nPROPN - 0.716410\nADV - 0.877637\nCCONJ - 0.991618\nPART - 0.970359\nNUM - 0.815675\nSCONJ - 0.841146\n_ - 0.991620\nSYM - 0.719424\nINTJ - 0.730159\nX - 0.000199\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-06-15T17:04:12.323450Z",
     "iopub.execute_input": "2024-06-15T17:04:12.324268Z",
     "iopub.status.idle": "2024-06-15T17:04:12.332187Z",
     "shell.execute_reply.started": "2024-06-15T17:04:12.324231Z",
     "shell.execute_reply": "2024-06-15T17:04:12.331281Z"
    },
    "trusted": true
   },
   "execution_count": 30,
   "outputs": [
    {
     "execution_count": 30,
     "output_type": "execute_result",
     "data": {
      "text/plain": "4963618"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inference\n",
    "\n",
    "Now let's look at some of the model's predictions."
   ],
   "metadata": {
    "cell_id": "80db8ab9f03c4c29a0b4e3bd597a7782",
    "deepnote_cell_type": "markdown",
    "id": "bI15KIqWoKK-"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "model = torch.load('pos_tagger.final.pt')\n",
    "\n",
    "def inference(sentence):\n",
    "    # Convert words to id tensor.\n",
    "    ids = [[dataset.token_vocab.word2id(x)] for x in sentence]\n",
    "    ids = Variable(torch.LongTensor(ids))\n",
    "    if torch.cuda.is_available():\n",
    "        ids = ids.cuda()\n",
    "    # Get model output.\n",
    "    output, _ = model(ids)\n",
    "    _, preds = torch.max(output, dim=2)\n",
    "    if torch.cuda.is_available():\n",
    "        preds = preds.cpu()\n",
    "    preds = preds.data.view(-1).numpy()\n",
    "    pos_tags = [dataset.pos_vocab.id2word(x) for x in preds]\n",
    "    for word, tag in zip(sentence, pos_tags):\n",
    "        print('%s - %s' % (word, tag))"
   ],
   "metadata": {
    "cell_id": "22433cbac35b46e1a0d8a86121da06ef",
    "deepnote_cell_type": "code",
    "id": "q6uOBHu8oKK-"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "sentence = \"sdfgkj asd;glkjsdg ;lkj  .\".split()\n",
    "inference(sentence)"
   ],
   "metadata": {
    "cell_id": "ee8488453add4dcab4e6d0e273f96d37",
    "deepnote_cell_type": "code",
    "id": "6HipFvTdoKK-"
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
